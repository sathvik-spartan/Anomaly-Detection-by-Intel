{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2019 Intel Corporation\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In Lesson 7 of *Anomaly Detection*, we discussed how to detect anomalies in time series using statistical process control and autoregressive models. Here we will apply both approaches to real-world datasets. \n",
    "\n",
    "\n",
    "# Learning Outcomes\n",
    "\n",
    "You should walk away from this Python tutorial with:\n",
    "\n",
    "1. An understanding of time series anomaly detection \n",
    "2. Practical experience with statistical process control and autoregressive models to detect anomalies in time series.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T06:32:41.931157Z",
     "start_time": "2018-12-16T06:32:40.024011Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#import warnings\n",
    "#warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "import sys\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import statsmodels as ss\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.arima_model import ARIMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python and library versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T06:32:41.938649Z",
     "start_time": "2018-12-16T06:32:41.933352Z"
    }
   },
   "outputs": [],
   "source": [
    "packages = [matplotlib, np, pd]\n",
    "\n",
    "msg = f\"\"\"\n",
    "Python Version: {sys.version}\n",
    "\n",
    "library .      version\n",
    "-------        -------\"\"\"\n",
    "print(msg)\n",
    "\n",
    "for package in packages:\n",
    "    print(f\"{package.__name__:11}    {package.__version__:>7}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Statistical process control\n",
    "\n",
    "In this section, we are going to use statistical process control (SPC) to detect anomaly in time series. As we discussed in lectures, SPC provides quality control for time series data. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, we will use precipitation data for October 2018 for New York City (collected at JFK airport). The raw data is available from the NOAA website:\n",
    "\n",
    "https://www.ncdc.noaa.gov/cdo-web/datasets/GHCND/stations/GHCND:USW00094789/detail\n",
    "\n",
    "Here we provide a clean version of the data. We made two changes to the raw data: (1) the dates have been written in a format useful for time series analysis and (2) trace precipitation values have been replaced with zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T06:32:41.967590Z",
     "start_time": "2018-12-16T06:32:41.941352Z"
    }
   },
   "outputs": [],
   "source": [
    "df_ppt = pd.read_csv('nyc_precipitation_oct2018.csv', parse_dates=True, index_col=0)\n",
    "df_ppt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The precipitation is given in inches. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See many days without precipitation and large values on October 11, 12 and 27. Visualize the data and summarize the statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T06:32:42.239330Z",
     "start_time": "2018-12-16T06:32:41.970880Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(dpi=120)\n",
    "df_ppt.plot(ax=plt.gca())\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T06:32:42.255392Z",
     "start_time": "2018-12-16T06:32:42.241787Z"
    }
   },
   "outputs": [],
   "source": [
    "df_ppt.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the many days without precipitation, the mean and standard deviation are fairly small, but the large maximum value suggest there is at least one anomalous point (October 11, 2018). \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Control plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the summary statistics and the plot that any anomalies will be above the mean, which is 0.107 in, so we need only a one-sided control plot.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is another way to reach the same conclusion. Since the precipitation cannot be a negative number, all possible values below the mean lie in the range 0 <= precipitation <= 0.107 in.), which are within one standard deviation (0.267 in.) of the mean and therefore not anomalies. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, let's create a one-sided control plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T06:32:42.263013Z",
     "start_time": "2018-12-16T06:32:42.257188Z"
    }
   },
   "outputs": [],
   "source": [
    "def control_plot(time_series, threshold):\n",
    "    '''\n",
    "    Creates a one-sided control plot from a time series\n",
    "    (that is, plots threshold above the mean but not below)\n",
    "    Also, returns list of points that exceed the threshold\n",
    "    i.e., points for which the value > mean + threshold*(standard deviation)\n",
    "    \n",
    "    Args: \n",
    "        time_series: (pandas dataframe; index column is date in datetime format and  \n",
    "        column 0 is data)\n",
    "        threshold: z-score threshold for anomaly detection (float)\n",
    "\n",
    "    Returns: \n",
    "        Control plot of time_series    \n",
    "        anomalies: anomalies that exceed threshold (pandas dataframe)\n",
    "    ''' \n",
    "    \n",
    "    mean_= time_series.iloc[:,0].mean()\n",
    "    stdev_= time_series.iloc[:,0].std()\n",
    "    cutoff = mean_+threshold*stdev_\n",
    "    plt.figure(dpi=140)\n",
    "    time_series.plot(ax=plt.gca())\n",
    "    plt.axhline(y=mean_, color='g', linestyle='--', label='mean')\n",
    "    # Use threshold to plot line at threshold*stdev_ times away from the mean\n",
    "    plt.axhline(y=cutoff, color='r', linestyle=':', label='threshold')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('NYC October 2018 precipitation')\n",
    "    plt.ylabel('Precipitation (in.)')\n",
    "    \n",
    "    # Create dataframe of anomalies that exceed the cutoff\n",
    "    anomalies = time_series[time_series.values > cutoff]\n",
    "    return anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set a threshold of 3 standard deviations and plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T06:32:42.600432Z",
     "start_time": "2018-12-16T06:32:42.264797Z"
    }
   },
   "outputs": [],
   "source": [
    "anomaly_ppt = control_plot(df_ppt, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's an anomaly. Get the data for this point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T06:32:42.608995Z",
     "start_time": "2018-12-16T06:32:42.602650Z"
    }
   },
   "outputs": [],
   "source": [
    "print(anomaly_ppt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result agrees with our visual inspection. (For small datasets, humans are good at detecting statistical patterns, such as outliers, by eye.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cumulative sum (CUSUM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison, let's apply the CUSUM approach and see what we find. We will use the algorithm outlines in the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T06:32:42.617210Z",
     "start_time": "2018-12-16T06:32:42.610983Z"
    }
   },
   "outputs": [],
   "source": [
    "def cusum(data, mean, shift, threshold):\n",
    "    '''\n",
    "    Calculate the high and low cumulative sums and use these for anomaly detection. \n",
    "    An anomaly is reported if the cumulative sums are beyong a given threshold.\n",
    "    \n",
    "    Args: \n",
    "        data: (a time series as pandas dataframe; index column is date in datetime format and  \n",
    "        column 0 is data)\n",
    "        mean:  mean of the data or other average (float)\n",
    "        shift: normal shift in the data; standard deviation is recommend (float)\n",
    "        threshold: threshold to classify point as anomaly (float)\n",
    "\n",
    "    Returns: \n",
    "        cusum: the high and low cumulative sums together with data (pandas dataframe)  \n",
    "        anomalies: anomalies that above and below threshold (pandas dataframe)\n",
    "    ''' \n",
    "    high_sum = 0.0\n",
    "    low_sum = 0.0\n",
    "    anomalies = [] \n",
    "    high_sum_final = []\n",
    "    low_sum_final = []\n",
    "    index_names = data.index\n",
    "    data_values = data.values\n",
    "    for index, item in enumerate(data_values):\n",
    "        high_sum = max(0, high_sum + item - mean - shift)\n",
    "        low_sum = min(0, low_sum + item - mean + shift)\n",
    "        high_sum_final.append(high_sum)\n",
    "        low_sum_final.append(low_sum)\n",
    "        if high_sum > threshold or low_sum < -threshold:\n",
    "            anomalies.append((index_names[index], item.tolist()))\n",
    "    cusum = data\n",
    "    cusum = cusum.assign(High_Cusum=high_sum_final, Low_Cusum=low_sum_final)\n",
    "    return cusum, anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we will also plot the results of our CUSUM analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T06:32:42.626144Z",
     "start_time": "2018-12-16T06:32:42.619706Z"
    }
   },
   "outputs": [],
   "source": [
    "def cusum_plot(time_series, threshold):\n",
    "    '''\n",
    "    Plot the high and low cumulative sums and use these for anomaly detection. \n",
    "    An anomaly is reported if the cumulative sums are beyong a given threshold.\n",
    "    \n",
    "    Args: \n",
    "        time_series: (a time series as pandas dataframe; index column is date \n",
    "        in datetime format and column 0 is data)\n",
    "        threshold: threshold to classify point as anomaly (float)\n",
    "\n",
    "    Returns: \n",
    "        A plot of the data with the high cumulative sum.\n",
    "        cusum_results: the high and low cumulative sums together with data \n",
    "        and any anomalies that above and below threshold (pandas dataframe; \n",
    "        from cumsum function)\n",
    "    ''' \n",
    "    # Use the mean and standard deviation of the whole time series \n",
    "    # to calculate cumulative sums\n",
    "    mean_= time_series.iloc[:,0].mean()\n",
    "    stdev_= time_series.iloc[:,0].std()\n",
    "    \n",
    "    # define threshold in terms of standard deviation\n",
    "    cusum_results = cusum(time_series, mean_, stdev_, threshold*stdev_)\n",
    "    ax=time_series.plot()\n",
    "    ax.axhline(y=mean_, color='g', linestyle='--',label='average')\n",
    "    ax.axhline(y=mean_+threshold*stdev_, color='r', linestyle='--',label='High threshold')\n",
    "    # Use threshold to plot line at threshold*stdev_ times away from the mean\n",
    "    ax.scatter(x=cusum_results[0].index, y=cusum_results[0]['High_Cusum'], \n",
    "             color='k', linestyle=':',label='High Cusum')                      \n",
    "    plt.legend(loc='center left')\n",
    "    plt.title('NYC October 2018 precipitation')\n",
    "    plt.ylabel('Precipitation (in.)')\n",
    "    plt.gcf().set_size_inches(8,6)\n",
    "    plt.show()\n",
    "    \n",
    "    return cusum_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choice of threshold: for the control chart, we used 3 times the standard deviation. Note that the equivalent threshold for CUSUM is *threshold*=2 since CUSUM includes the *shift* (which we chose to be one standard deviation) in its calculation of the cumulative sum. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T06:32:42.871983Z",
     "start_time": "2018-12-16T06:32:42.628284Z"
    }
   },
   "outputs": [],
   "source": [
    "cusum_plot(df_ppt, 2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that CUSUM finds the same anomaly as the control chart (1.03 inches on October 11, 2018), but it also labels the two successive points as anomalies. Why? \n",
    "\n",
    "The different results reflect the difference in the algorithms. The control chart is looking for single, anomalous points. CUSUM is sensitive to change in the behavior of the data. As a result, it flags points as anomalous until the behavior of the time series has returned to normal. \n",
    "\n",
    "Indeed, CUSUM is used for change point detection: finding when the underlying distribution of the time series has changed. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A final comment.**  We used all of the data to compute the baseline statistics (mean and standard deviation) for the anomaly detection. While this approach is sometimes used, the presence of anomalies in the data can bias these baseline statistics. A more rigorous approach is to select data that is considered normal, use that data to calculate the baseline statistics and only then detect the anomalies.  We will explore this idea in Exercise 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Autogressive models\n",
    "\n",
    "So far, we carried out analysis in offline mode---we had all of the data of interest at hand. As a result, when examining any given point, we could use both the past and the future (with respect to that point) to calculate statistics such as the mean and standard deviation.\n",
    "\n",
    "Often, we are interested in analyzing time series in streaming mode---that is, as the data becomes available. In this case, any models we use to detect anomalies can only use the past (with respect to a given point). Future data is not available. \n",
    "\n",
    "As discussed in lectures, autoregressive models are commonly used for streaming anomaly detection and we will examine one such model below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use another weather dataset from New York City (collected at JFK airport): maximum and minimum daily temperatures from February 1, 1959 to October 31, 2018. \n",
    "\n",
    "The raw data is available from the NOAA website we used:\n",
    "\n",
    "https://www.ncdc.noaa.gov/cdo-web/datasets/GHCND/stations/GHCND:USW00094789/detail\n",
    "\n",
    "Here we provide a clean version of the data. We processed the raw data as follows: (1) the dates have been written in a format useful for time series analysis and (2) two missing values have been filled in.\n",
    "\n",
    "The two dates that had no values (for either the maximum or minimum temperature) were Dec. 1, 1959 and May 1, 1960. For the purposes of this notebook, we added anomalous values to these dates as shown below. (If we weren't studying anomaly detection, we would have added values corresponding to the mean temperatures of the day before and day after to avoid gaps in the time series.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T06:32:44.842781Z",
     "start_time": "2018-12-16T06:32:42.873981Z"
    }
   },
   "outputs": [],
   "source": [
    "df_nyc_temp = pd.read_csv('nyc_daily_temp_final.csv', parse_dates=True, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T06:32:45.653187Z",
     "start_time": "2018-12-16T06:32:44.844415Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(dpi=120)\n",
    "df_nyc_temp.plot(ax=plt.gca())\n",
    "plt.title('Daily NYC temp')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Temp (°F)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not easy to see whether there are anomalies with so many points. Let's look at the statistics for both TMAX (the maximum daily temperature) and TMIN (the minimum daily temperature). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T06:32:45.674777Z",
     "start_time": "2018-12-16T06:32:45.655338Z"
    }
   },
   "outputs": [],
   "source": [
    "df_nyc_temp.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The highest value of TMAX (104 °F) and the lowest value of TMIN (-2 °F) are quite extreme. But are these anomalies?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And what about the anomalies we seeded into the dates with missing values?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Dec. 1, 1959, TMAX and TMIN are 40 °F *warmer* than what a linear interpolation would predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T06:32:45.688537Z",
     "start_time": "2018-12-16T06:32:45.677180Z"
    }
   },
   "outputs": [],
   "source": [
    "df_nyc_temp.loc['1959-11-30':'1959-12-02']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For May 1, 1960, TMAX and TMIN are 40°F *colder* than what a linear interpolation would predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T06:32:45.704241Z",
     "start_time": "2018-12-16T06:32:45.690675Z"
    }
   },
   "outputs": [],
   "source": [
    "df_nyc_temp.loc['1960-4-30':'1960-5-02']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will we detect these points as anomalies?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use an autoregressive model to detect anomalies. Here we will look at TMAX and leave TMIN for Exercise 2. \n",
    "\n",
    "We start by creating a dataframe of just TMAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T06:32:45.723247Z",
     "start_time": "2018-12-16T06:32:45.711028Z"
    }
   },
   "outputs": [],
   "source": [
    "df_nyc_tmax=df_nyc_temp[['TMAX']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the new dataframe is as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T06:32:45.747657Z",
     "start_time": "2018-12-16T06:32:45.735100Z"
    }
   },
   "outputs": [],
   "source": [
    "df_nyc_tmax.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T06:32:45.769138Z",
     "start_time": "2018-12-16T06:32:45.756464Z"
    }
   },
   "outputs": [],
   "source": [
    "df_nyc_tmax.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a baseline for the autoregression model, we can check what a control plot reveals as anomalies. In contrast to the precipitation example, here we can have anomalies on either side of the mean, so we modify *control_plot* to become a full, two-sided control plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T06:32:45.784408Z",
     "start_time": "2018-12-16T06:32:45.771344Z"
    }
   },
   "outputs": [],
   "source": [
    "def control_plot_full(time_series, threshold):\n",
    "    '''\n",
    "    Creates a two-sided control plot from a time series\n",
    "    (plots threshold above and below the mean)\n",
    "    Also, returns list of points that exceed the threshold\n",
    "    i.e., points for which the value > mean + threshold*(standard deviation) \n",
    "    and value < mean - threshold*(standard deviation)\n",
    "    \n",
    "    Args: \n",
    "        time_series: (pandas dataframe; index column is date in datetime format and  \n",
    "        column 0 is data)\n",
    "        threshold: threshold for anomaly detection (float)\n",
    "\n",
    "    Returns: \n",
    "        Control plot of time_series    \n",
    "        anomalies: anomalies that exceed threshold (pandas dataframe)\n",
    "    ''' \n",
    "    \n",
    "    mean_= time_series.iloc[:, 0].mean()\n",
    "    stdev_= time_series.iloc[:, 0].std()\n",
    "    time_series.plot()\n",
    "    plt.axhline(y=mean_, color='g', linestyle='--',label='average')\n",
    "    # Use threshold to plot line at threshold*stdev_ times away from the mean\n",
    "    plt.axhline(y=mean_+threshold*stdev_, color='r', linestyle=':', label='high threshold')\n",
    "    plt.axhline(y=mean_-threshold*stdev_, color='m', linestyle=':', label='low threshold')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('NYC Temperature')\n",
    "    plt.ylabel('Temp (F)')\n",
    "    plt.gcf().set_size_inches(8,6)\n",
    "    plt.show()\n",
    "    \n",
    "    # Create dataframe of anomalies that exceed the threshold\n",
    "    anomaly_mask = (np.abs(time_series.values - mean_) > threshold*stdev_)\n",
    "    anomalies = time_series[anomaly_mask]\n",
    "    return anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T06:32:46.278327Z",
     "start_time": "2018-12-16T06:32:45.787202Z"
    }
   },
   "outputs": [],
   "source": [
    "control_plot_full(df_nyc_tmax, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find only one anomaly: the minimum of TMAX. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On to our autoregression model. An overview of such models (with references for further study) is given in the lecture; a detailed discussion goes beyond the scope of the lessons on anomaly detection. \n",
    "\n",
    "**Note_1**: It can take a significant amount of work to find a successful autoregression model and we do not claim that the model presented here is optimal.The parameters were chosen to have a reasonable runtime.\n",
    "\n",
    "**Note_2**: There are models that explicit incorporate _seasonal effects_ while modeling, such as the statsmodels SARIMAX function. The length of a full cycle cannot be too long for the series to converge, so yearly seasons don't work well on daily data (each season is 365 data points). A more sophisticated treatment, such as doing a Fourier Analysis and then subtracting off the annual component would typically be done in this case. \n",
    "\n",
    "**Note_3**: The autoregression models in python require that the time series analyzed have an appropriate date frequency, which is why it is important to check for missing values as a pre-processing step. If the time_series is complete, many packages that deal with time_series can infer the frequency, even if it isn't supplied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T06:32:46.290287Z",
     "start_time": "2018-12-16T06:32:46.281067Z"
    }
   },
   "outputs": [],
   "source": [
    "print(df_nyc_tmax.index.inferred_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Daily frequency. That's good! Let's make it explicit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T06:32:46.303411Z",
     "start_time": "2018-12-16T06:32:46.298138Z"
    }
   },
   "outputs": [],
   "source": [
    "df_nyc_temp.index.freq=df_nyc_tmax.index.inferred_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now to fit our data with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T06:32:55.747363Z",
     "start_time": "2018-12-16T06:32:46.305445Z"
    }
   },
   "outputs": [],
   "source": [
    "tmax_sar=ARIMA(df_nyc_tmax, order=(1,1,4)).fit()\n",
    "\n",
    "tmax_sar.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will not discuss the statistics reported, except to say that it includes the standard deviation of the residuals (6.516), which we will use later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check the diagnostics to check that the assumptions underlying the model are met and also for additional information on the quality of the fit.  This is done using a Q-Q plot (checking that the residuals follow a normal distribution), investigating the residuals for temporal patterns, and plotting a histogram of the residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T06:32:56.076359Z",
     "start_time": "2018-12-16T06:32:55.750391Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get the predicted standard deviation. This is the 6.516 we saw earlier\n",
    "sigma_pred = tmax_sar.resid.std()\n",
    "# Calculate the standardized residuals from the (regular) residuals\n",
    "tmax_std_resid = tmax_sar.resid/sigma_pred\n",
    "\n",
    "plt.title('Patterns in residual')\n",
    "plt.plot(tmax_std_resid);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T06:32:56.610146Z",
     "start_time": "2018-12-16T06:32:56.077849Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,6))\n",
    "ax = plt.subplot(121)\n",
    "plt.title('Distribution of residuals')\n",
    "sns.distplot(tmax_std_resid.values, bins=50, ax=ax);\n",
    "stats.probplot(tmax_std_resid.values, dist='norm', sparams=(2.5,), plot=plt.subplot(122));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For anomaly detection, we focus on the top plot: standardized residuals. The standardized residual is the residual (the difference between the observed value and the predicted value) divided by the predicted standard deviation (the square root of the predicted variance we mentioned above). It is a more sophisticated version of the z-score we discussed in Lesson 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A rule of thumb for detecting anomalies with standardized residuals: anomalies are points for which the magnitude of the standardized residals is greater than 4. Let's find these points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T06:32:56.617817Z",
     "start_time": "2018-12-16T06:32:56.612193Z"
    }
   },
   "outputs": [],
   "source": [
    "# Report the anomalies\n",
    "anomaly_mask = np.abs(tmax_std_resid) > 4\n",
    "tmax_anomalies = tmax_std_resid[anomaly_mask]\n",
    "print(tmax_anomalies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model found both of the seeded anomalies (Dec. 1, 1959 and May 1, 1960). \n",
    "\n",
    "The model did not find the lone anomaly detected by the control plot (January 17, 1977), but it did find several others that were not picked up by the control plot. \n",
    "\n",
    "The next step would be to vary the parameters of the autoregression model and check how robust these findings are, but as that belongs to the realm of time series analysis, we will stop here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise #1\n",
    "\n",
    "This exercise refers to Section 1 (statistical process control). \n",
    "\n",
    "A. Choose a subset of the precipitation data that you consider anomaly-free (for example, the first 10 days). Use this data to calculate the mean and standard deviation.\n",
    "\n",
    "B. Repeat the control chart analysis.\n",
    "\n",
    "C. Compare the anomalies you find with those found previously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise #2\n",
    "\n",
    "This exercise refers to Section 2 (autogression models). \n",
    "\n",
    "A. Repeat the analysis for the TMIN values. \n",
    "\n",
    "B. Do you find the same anomalies as TMAX?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution #2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "In this assignment you should have learned: \n",
    "\n",
    "1. An understanding of time series anomaly detection \n",
    "2. Practical experience with statistical process control and autoregressive models to detect anomalies in streaming data.\n",
    "\n",
    "Congratulations! This concludes the lesson."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Intel Anomaly Env",
   "language": "python",
   "name": "intel_anomaly"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
