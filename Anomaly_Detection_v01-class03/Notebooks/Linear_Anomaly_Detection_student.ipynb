{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2019 Intel Corporation\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In Lesson 3 of *Anomaly Detection: Linear Methods*, we learned about linear methods used for anomaly detection: linear regression, principal component analysis (PCA) and one-class support vector machines (SVMs).\n",
    "\n",
    "Here will apply all three techniques to detect anomalies in synthetic and real-world data.\n",
    "\n",
    "# Learning Outcomes\n",
    "\n",
    "You should walk away from this Python tutorial with:\n",
    "1. Practical experience with linear regression models\n",
    "2. Practical experience with PCA\n",
    "3. Practical experience with one-class SVMs \n",
    "\n",
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-30T03:30:09.580304Z",
     "start_time": "2018-12-30T03:30:08.314583Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(suppress=True, precision=4)\n",
    "\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.decomposition as decomp\n",
    "import sklearn.linear_model as linear_model\n",
    "import sklearn.datasets as sk_data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy.linalg as nla\n",
    "import sklearn.svm as svm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python and library versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-30T03:30:09.615897Z",
     "start_time": "2018-12-30T03:30:09.601717Z"
    }
   },
   "outputs": [],
   "source": [
    "packages = [np]\n",
    "\n",
    "msg = f\"\"\"\n",
    "Python Version: {sys.version}\n",
    "\n",
    "library .      version\n",
    "-------        -------\"\"\"\n",
    "print(msg)\n",
    "\n",
    "for package in packages:\n",
    "    print(f\"{package.__name__:11}    {package.__version__:>7}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Linear regression models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we are going to use synthetic datasets to detect  anomalies using a linear regression. We'll apply what we've learned to a real-world dataset in the exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key idea: deviation from regression model--that is, the difference between the actual value and the prediction value--is a measure of how anomalous a point is.\n",
    "\n",
    "A reminder of how linear regression models is used for anomaly detection: \n",
    "\n",
    "1. Split data into train and test datasets\n",
    "2. Use train data sets to (i) get the parameters of the regression model and (ii) determine the threshold for anomalies\n",
    "3. Apply the results to the test data to detect anomalies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T20:44:29.406537Z",
     "start_time": "2018-12-14T20:44:29.402932Z"
    }
   },
   "source": [
    "Here we will work with the data shown in the lecture: exam grade and hours studied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-30T03:30:10.881416Z",
     "start_time": "2018-12-30T03:30:10.877242Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Approximately linear data\n",
    "exam_data1 = np.array([[1, 2, 3, 4, 5],\n",
    "                    [57, 70, 76, 84, 91]]).T\n",
    "print(exam_data1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each pair of data points, the first value is the hours studies and the second is the exam grade (out of 100)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-30T03:30:11.409716Z",
     "start_time": "2018-12-30T03:30:11.405364Z"
    }
   },
   "outputs": [],
   "source": [
    "# One anomaly replaces a normal point\n",
    "exam_data2 = np.array([[1, 2, 3, 4, 5],\n",
    "                      [57, 70, 99, 84, 91]]).T\n",
    "print(exam_data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The anomaly is (3, 99)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify our work, we will introduce a helper function that plots a straight line given the slope, intercept, axes (to create the figure) and the style of the line (to plot multiple lines on the same graph)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-30T03:30:12.124927Z",
     "start_time": "2018-12-30T03:30:12.121252Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_mb(m, b, ax, style):\n",
    "    'plot a line y=mx+b on a matplotlib axis'\n",
    "    xs = ax.get_xlim()\n",
    "    ax.plot(xs, m*xs + b, style) #style is type of line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the two datasets together so we can easily compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-30T03:30:12.981715Z",
     "start_time": "2018-12-30T03:30:12.736179Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2,sharex=True)\n",
    "\n",
    "# Modify axes so they look nice\n",
    "axes[0].set_xlim([0, 6.01])\n",
    "start_x, end_x = axes[0].get_xlim()\n",
    "stepsize_x = 1\n",
    "axes[0].xaxis.set_ticks(np.arange(start_x, end_x, stepsize_x))\n",
    "axes[0].yaxis.set_ticks_position('both')\n",
    "axes[0].set_ylim([45, 108])\n",
    "axes[1].set_ylim(axes[0].get_ylim())\n",
    "axes[1].yaxis.tick_right() # Display tick values on the right for clarity\n",
    "axes[1].yaxis.set_ticks_position('both')\n",
    "\n",
    "# Fit a straight straight line to the linear data\n",
    "lr = linear_model.LinearRegression().fit(exam_data1[:,0:1],\n",
    "                                         exam_data1[:,1])\n",
    "axes[0].plot(*exam_data1.T, 'ro')\n",
    "plot_mb(lr.coef_, lr.intercept_, axes[0], 'b-' )\n",
    "\n",
    "# Fit a straight straight line to data with anomaly\n",
    "lr2 = linear_model.LinearRegression().fit(exam_data2[:,0:1],\n",
    "                                         exam_data2[:,1])\n",
    "axes[1].plot(*exam_data2.T, 'ro')\n",
    "\n",
    "# Plot two linear fits: from data with anomaly and from normal data\n",
    "plot_mb(lr2.coef_, lr2.intercept_, axes[1], 'b-')\n",
    "plot_mb(lr.coef_, lr.intercept_, axes[1], 'g--')\n",
    "\n",
    "axes[0].set_title('No anomaly')\n",
    "axes[1].set_title('With anomaly')\n",
    "fig.text(0.5, 0.02, 'Hours studied', ha='center', va='center')\n",
    "fig.text(0.05, 0.5, 'Grade', ha='center', va='center', rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, all of the data on the left lies close to the regression model (blue line), while for the data on the right we can see that there is a point that is far from the line. This point (3, 99) is the anomaly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** There are two lines in the panel on the right: the regression model for the data with the anomaly (solid blue line) and the one found from the normal data from the left panel (dashed green line). The difference between these two lines is due to the anomaly itself (all other points are unchanged). That is, anomalies affect the regression model.\n",
    "\n",
    "It is because anomalies affect the regression model that we do the train/test split to ensure that the linear fit reflects only the normal data. Of course, such an approach presumes that we can have (or can create) a training set with only normal data.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now treat 'exam_data1' (no anomaly) as the train dataset and 'exame_data2' (with anomaly) as the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we fit our train dataset. We will use the _LinearRegression_ function from_sklearn_ as we did above. Recall from the anomaly score is the square of the residual for each point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-30T03:30:13.656912Z",
     "start_time": "2018-12-30T03:30:13.648689Z"
    }
   },
   "outputs": [],
   "source": [
    "ftrs, tgt = exam_data1[:,0:1], exam_data1[:,1]\n",
    "lr_train = linear_model.LinearRegression().fit(ftrs, tgt)\n",
    "print(f'Slope: {lr_train.coef_}')\n",
    "print(f'Intercept: {lr_train.intercept_:.{3}}')\n",
    "train_scores = (tgt - lr_train.predict(ftrs))**2 \n",
    "print(train_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set the threshold for anomaly detection to be the just above the maximum score from the train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-30T03:30:14.271769Z",
     "start_time": "2018-12-30T03:30:14.265280Z"
    }
   },
   "outputs": [],
   "source": [
    "margin = 0.01\n",
    "threshold = max(train_scores) + margin\n",
    "print(f'Threshold: {threshold:.{3}}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's calculate the anomaly scores for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-30T03:30:14.845352Z",
     "start_time": "2018-12-30T03:30:14.840990Z"
    }
   },
   "outputs": [],
   "source": [
    "def do_linreg_anomaly_scores(train, test):\n",
    "    ftrs, tgt = train[:,0:1], train[:,1]\n",
    "    lr_train = linear_model.LinearRegression().fit(ftrs, tgt)\n",
    "    anom_score = (test[:,1] - lr_train.predict(test[:,0:1]))**2\n",
    "    return anom_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-30T03:30:15.216805Z",
     "start_time": "2018-12-30T03:30:15.211439Z"
    }
   },
   "outputs": [],
   "source": [
    "print(do_linreg_anomaly_scores(exam_data1, exam_data2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the middle point (index=2) exceeds the threshold. It is the anomaly we introduced into the data. As a check, we can compare the anomaly score above with those of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-30T03:30:20.367212Z",
     "start_time": "2018-12-30T03:30:20.335532Z"
    }
   },
   "outputs": [],
   "source": [
    "print(train_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the two sets of scores differ only for the anomalous point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Principal component analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we are going to use synthetic datasets to detect  anomalies using PCA. We'll apply what we've learned to a real-world dataset in the exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to calculate PCA-based anomaly scores:\n",
    "\n",
    "  1. Preprocess the data (if needed)\n",
    "  2. Compute the principal components (PCs) of the centered data\n",
    "  3. Project our examples onto the PCs\n",
    "  4. Calculate the distance between the original and the projected examples\n",
    "  5. Use the distance to score the anomalies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with the data shown in the lecture: a simple 2D dataset of synthetic data: ($X_1, X_2$). To demonstrate how PCA works, we take $X_2 = X_1/2$ for all pairs except one. where we made a small change: (0.5, 0.26). This point is the anomaly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-30T03:31:07.210255Z",
     "start_time": "2018-12-30T03:31:07.205787Z"
    }
   },
   "outputs": [],
   "source": [
    "pca_example = np.array([[-3, -1.5], [-2.5, -1.25], [-1.5, -0.75], \n",
    "                        [-1, -0.5], [-0.5, -0.25], [0, 0], [0.5, 0.26], \n",
    "                        [1, 0.5],  [1.5, 0.75], [2.5, 1.25], [3, 1.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the mean of the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-30T03:31:07.989234Z",
     "start_time": "2018-12-30T03:31:07.980416Z"
    }
   },
   "outputs": [],
   "source": [
    "mean_pca_example = np.mean(pca_example, axis=0, keepdims=True)\n",
    "mean_pca_example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the variance? We want to make sure that the variance in each direction is approximately equal (otherwise something trivial, like changing the unit of measurement from meters to millimeters, will drastically change the principal components)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-30T03:31:08.737450Z",
     "start_time": "2018-12-30T03:31:08.732215Z"
    }
   },
   "outputs": [],
   "source": [
    "var_pca_example = np.var(pca_example, axis=0, keepdims=True)\n",
    "var_pca_example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since mean of this data is (practically) zero, we don't need to do mean subtraction. We should correct for the different variances, however. The typical approach is to divide each component by its standard deviation, enforcing the variance in each direction is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-30T03:31:09.605497Z",
     "start_time": "2018-12-30T03:31:09.594252Z"
    }
   },
   "outputs": [],
   "source": [
    "scaled_pca_example = pca_example/np.sqrt(var_pca_example)\n",
    "\n",
    "# show the variances are equal\n",
    "scaled_pca_example.var(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the original (i.e. _unscaled_) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-30T03:31:10.866286Z",
     "start_time": "2018-12-30T03:31:10.526272Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(pca_example[:,0], pca_example[:,1])\n",
    "ax.set_ylabel('$X_2$')\n",
    "ax.set_xlabel('$X_1$')\n",
    "ax.set_title('Original data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you spot the anomaly?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show what PCA can do, we will apply it to our dataset and take a sneak peek at the results (using PCA from *sklearn*). We expect the first principal component to lie along the $X_2 = X_1/2$ line because that is the direction along which the data varies the most. And given the way we constructed the dataset, there will be almost no variation along the second principal component except for the anomaly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-30T03:31:12.612426Z",
     "start_time": "2018-12-30T03:31:12.602387Z"
    }
   },
   "outputs": [],
   "source": [
    "pca = decomp.PCA(n_components=2)\n",
    "pca.fit(scaled_pca_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-30T03:31:14.541582Z",
     "start_time": "2018-12-30T03:31:14.388748Z"
    }
   },
   "outputs": [],
   "source": [
    "pca_example_trf = pca.transform(scaled_pca_example) \n",
    "fig2, ax2 = plt.subplots()\n",
    "ax2.scatter(pca_example_trf[:,0], pca_example_trf[:,1])\n",
    "ax2.set_ylabel('$PC2$')\n",
    "ax2.set_xlabel('$PC1$')\n",
    "ax2.set_title('Principal components')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The anomaly is evident."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll proceed more formally. We create a function to carry out PCA on 2D data following the five steps we outlined at the beginning of this section.\n",
    "\n",
    "Instead of doing the centering and mean normalization ourselves, we will use the [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) built into scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-30T03:31:16.328547Z",
     "start_time": "2018-12-30T03:31:16.322155Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_1d_projected_vectors(obs, pca_object):\n",
    "    # Note: The projection of vector a (data) along vector b (PC1)\n",
    "    # is given by  [b / len(b)]* (len (a) cos (theta))\n",
    "    # where theta is the angle between and b and the term in \n",
    "    # square parenthesis is a unit vector in the b direction\n",
    "    #\n",
    "    # Since cos (theta) = dot(a,b)/(len(a)len(b))\n",
    "    # the projection can be written as\n",
    "    # projs = b*[dot(a,b)/len(b)^2]\n",
    "    #\n",
    "    # The term in square parethenses is y_lengths\n",
    "    # The projs is calculated adding back the mean\n",
    "    # subtracted previously to center the data\n",
    "    #\n",
    "    # This is a very explicit way of handling the calculation.\n",
    "    # See notes in \"higher dimension\" to see a way of generalizing\n",
    "    # this to higher dimensions, while encapsulating the vector math.\n",
    "    ssX = StandardScaler()\n",
    "    centered_data = ssX.fit_transform(obs)\n",
    "    pca_dirs = pca_object.components_\n",
    "    \n",
    "    y_lengths = centered_data.dot(pca_dirs.T) / pca_dirs.dot(pca_dirs.T)\n",
    "    centered_projs = y_lengths*(pca_dirs)\n",
    "    \n",
    "    # Return the data to its original uncentered (and unscaled) positions\n",
    "    return ssX.inverse_transform(centered_projs)\n",
    "\n",
    "def do_pca_anomaly_scores(obs, pca_object):\n",
    "    projected_vectors = get_1d_projected_vectors(obs, pca_object)    \n",
    "    return nla.norm(obs-projs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-30T03:31:17.414823Z",
     "start_time": "2018-12-30T03:31:17.406072Z"
    }
   },
   "outputs": [],
   "source": [
    "def do_1d_pca_anomaly_scores(obs):\n",
    "    fig, ax = plt.subplots(figsize=(6,6))\n",
    "    ax.set_ylabel('$X_2$')\n",
    "    ax.set_xlabel('$X_1$')\n",
    "    ax.set_title('Original data with PCA')\n",
    "    \n",
    "    # draw data\n",
    "    ax.scatter(*obs.T, label='data')\n",
    "\n",
    "    # Step 1: center and scale the data\n",
    "    ssX = StandardScaler()\n",
    "    centered_data = ssX.fit_transform(obs)\n",
    "    mean = ssX.mean_\n",
    "    \n",
    "    #for completeness, show mean on plot\n",
    "    ax.scatter(*mean.T, c='k', marker='^', label='mean') \n",
    "\n",
    "    # Step 2: compute prinicpal components\n",
    "    # Here we focus on first PC  (greatest proportion of variance)\n",
    "    pca = decomp.PCA(n_components=1)\n",
    "    pca.fit_transform(centered_data)\n",
    "    pca_dirs = pca.components_\n",
    "\n",
    "    # draw principal components\n",
    "    pca_endpoints = np.r_[-3.5*ssX.inverse_transform(pca_dirs),\n",
    "                           3.5*ssX.inverse_transform(pca_dirs)]\n",
    "    ax.plot(*pca_endpoints.T, 'y', label='PC1')\n",
    "\n",
    "    # Step 3: Project our examples onto the PCs\n",
    "    # \n",
    "    projs = get_1d_projected_vectors(obs, pca)\n",
    "    ax.plot(*projs.T, 'r.', label='projected data')\n",
    "    ax.legend(loc='best')\n",
    "    \n",
    "    # Step 4: Calculate distance between original and projected examples\n",
    "    # Step 5: Use the distance to score the anomalies\n",
    "    # The distance is the Euclidean norm and \n",
    "    # we use it as the anomaly score\n",
    "    return nla.norm(obs-projs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-30T03:31:18.808642Z",
     "start_time": "2018-12-30T03:31:18.600403Z"
    }
   },
   "outputs": [],
   "source": [
    "pca_example_scores = do_1d_pca_anomaly_scores(pca_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot shows that data (blue), the data projected onto PC1 (red points on yellow line) and the mean of the data used for centering (black triangle). By construction, PC1 passes through the mean, which is located at (0,0)--hard to see because there are a data point and a projected data point there too. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the data lies very close to PC1, it is hard to see the anomaly. Therefore, we look at the anomaly scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-30T03:31:20.293825Z",
     "start_time": "2018-12-30T03:31:20.287985Z"
    }
   },
   "outputs": [],
   "source": [
    "print(pca_example_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see one score (0.0101) which is much larger than the others. To what point does it correspond? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-30T03:31:21.412509Z",
     "start_time": "2018-12-30T03:31:21.408406Z"
    }
   },
   "outputs": [],
   "source": [
    "print(pca_example[np.argmax(pca_example_scores)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is the anomaly in the data. PCA worked!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We repeat the analysis with noisier data: two Gaussian clusters seeded with two anomalies at (6.0, 6.0) and (0.0, 10.0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-30T03:31:23.531018Z",
     "start_time": "2018-12-30T03:31:23.324434Z"
    }
   },
   "outputs": [],
   "source": [
    "blobs_X, y = sk_data.make_blobs(centers=[[0,0], [10,10]])\n",
    "figure, axes = plt.subplots(figsize=(6,6))\n",
    "axes.scatter(*blobs_X.T, c=y)\n",
    "\n",
    "spike_1 = np.array([[6.0,6.0]]) # Anomaly 1\n",
    "spike_2 = np.array([[0.0,10]])  # Anomaly 2\n",
    "axes.scatter(*spike_1.T, c='r')\n",
    "axes.scatter(*spike_2.T, c='g')\n",
    "axes.set_aspect('equal')\n",
    "axes.set_ylabel('$X_2$')\n",
    "axes.set_xlabel('$X_1$')\n",
    "axes.set_title('Original cluster data with two anomalies')\n",
    "\n",
    "# Combine the data so that the last two points are the anomalies\n",
    "cluster_data = np.concatenate([blobs_X, spike_1, spike_2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carry out PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-30T03:31:24.460524Z",
     "start_time": "2018-12-30T03:31:24.233066Z"
    }
   },
   "outputs": [],
   "source": [
    "cluster_data_scores = do_1d_pca_anomaly_scores(cluster_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this dataset, the difference between the original data and the projected data is apparent--only some of the variation in the data can be explained by projecting it onto a line. (Note that the mean (black triangle) gets projected onto PC1 as well.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the anomaly scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-30T03:31:25.566324Z",
     "start_time": "2018-12-30T03:31:25.558797Z"
    }
   },
   "outputs": [],
   "source": [
    "print(cluster_data_scores)\n",
    "print(cluster_data_scores.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The point with the highest scored is one of our seeded anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-30T03:31:26.417676Z",
     "start_time": "2018-12-30T03:31:26.413768Z"
    }
   },
   "outputs": [],
   "source": [
    "print(max(cluster_data_scores))\n",
    "print(cluster_data[np.argmax(cluster_data_scores)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the way we constructed our dataset, we know that the last two points are the anomalies. \n",
    "\n",
    "The one at (0.0, 10.0), which has index=101, is the one we found above. \n",
    "\n",
    "The other one (6.0, 6.0), with index=100, has an anomaly score of 0.0886, which is very low. This is because it lies very close to PC1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example shows the limitations of PCA for anomaly detection with clustered data. For such data, proximity methods should also be tried (see lesson 4). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Higher dimensions\n",
    "\n",
    "In the function `do_pca_anomaly_scores` we called the helper function `get_projected_vectors` which found the projection of the vectors in our dataset onto the principal direction. By subtracting the projected vector from the original vector, we could find the length of the orthogonal components that we used to score. \n",
    "\n",
    "The approach used worked with one principal direction, and relied heavily on vector math. We can ask the (fitted) `pca` object to do the heavy lifting for us:\n",
    "1. `reduced = pca.tranform(X)` will take our $N$-dimensional vectors, and project them onto the $d$-dimensional subspace. In our example, $N=2$ and $d=1$.\n",
    "2. `projected = pca.inv_transform(reduced)` will embed the reduced vectors back into our $N$-dimensional subspace\n",
    "3. `X - projected` are the components of the vectors that don't lie in our subspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-30T03:31:27.917335Z",
     "start_time": "2018-12-30T03:31:27.912315Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_projected_vectors(X, pca, ssX=None):\n",
    "    if not ssX:\n",
    "        ssX = StandardScaler().fit(X)\n",
    "    centered_data = ssX.transform(X)\n",
    "    reduced = pca.transform(centered_data)\n",
    "    # To get back to the original space, we need to undo the PCA\n",
    "    # as well as undo the scaling/centering step.\n",
    "    return ssX.inverse_transform(pca.inverse_transform(reduced))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-30T03:31:28.193425Z",
     "start_time": "2018-12-30T03:31:28.181595Z"
    }
   },
   "outputs": [],
   "source": [
    "# Show that it does the same thing as the previous version\n",
    "def do_pca_anomaly_scores(obs, n_components=1):\n",
    "    ssX = StandardScaler()\n",
    "    centered_data = ssX.fit_transform(obs)\n",
    "    pca = decomp.PCA(n_components=n_components)\n",
    "    pca.fit(centered_data)\n",
    "    \n",
    "    projected_vectors = get_projected_vectors(obs, pca)\n",
    "    return nla.norm(obs - projected_vectors, axis=1)\n",
    "\n",
    "# check that the two answers agree\n",
    "do_pca_anomaly_scores(pca_example) - pca_example_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our higher dimensional analog returns the same result as doing the vector arithmetic manually!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: One-class support vector machine (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will use the cluster dataset we created in the previous section to detect anomalies using one-class SVM. We'll apply what we've learned to a real-world dataset in the exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we discussed in the lecture, one-class SVM tries to find the hyperplane with the largest separation between the normal and anomaly classes. Typically, the data is not used as is, but transformed using the \"kernel trick\" (see lecture). \n",
    "\n",
    "We will use the one-class SVM implemented in *sklearn*, which uses the radial basis function (RBF) by default. The input for the RBF kernel is the parameter *gamma* and we use its default value as well.\n",
    "(A discussion of kernel is beyond the scope of this lesson. For more information, see\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html#sklearn.svm.OneClassSVM)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-class SVM typically returns anomalies (-1) or normal points (+1).  We, however, are interested in scoring our points and then ranking them by score.  Therefore, we will use the `decision_function` provided which returns the signed distance to separating hyperplane (negative distances are anomalies). To be consistent with out previous convention, where larger positive scores reflect more anomalous points, we need to take the negative of the `decision_function` ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-30T03:31:29.772256Z",
     "start_time": "2018-12-30T03:31:29.767698Z"
    }
   },
   "outputs": [],
   "source": [
    "def do_svm_anomaly_scores(obs):\n",
    "    \n",
    "    oc_svm = svm.OneClassSVM(gamma='auto').fit(obs)\n",
    "    scores = oc_svm.decision_function(obs).flatten()\n",
    "    \n",
    "    # Find the largest score and use it to normalize the scores\n",
    "    max_score = np.max(np.abs(scores))\n",
    "    \n",
    "    # scores from oc_svm use \"negative is anomaly\"\n",
    "    # To follow our previous convention\n",
    "    # we multiply by -1 and divide by the maximum score to get scores\n",
    "    # in the range [-1, 1] with positive values indicating anomalies\n",
    "    return - scores / max_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T20:45:22.704247Z",
     "start_time": "2018-12-14T20:45:22.696715Z"
    }
   },
   "source": [
    "Apply the one-class SVM to the cluster dataset of the previous section.\n",
    "Look at both the raw scores and the top five points (by score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-30T03:31:30.481233Z",
     "start_time": "2018-12-30T03:31:30.473156Z"
    }
   },
   "outputs": [],
   "source": [
    "print(do_svm_anomaly_scores(cluster_data))\n",
    "print (do_svm_anomaly_scores(cluster_data).argsort()[-5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two seeded anomalies (index=100 and 101) are the two highest ranked points. The performance is better than PCA, but the process of anomaly detection is less transparent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all of the exercises, we will use a real-world dataset: the `ionosphere` dataset from the UCI Machine Learning Database repository. \n",
    "This dataset consists of radar returns from the ionosphere and was originally used to classify the returns as good (suitable for further research) or bad. More information is available here: https://archive.ics.uci.edu/ml/datasets/ionosphere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of this notebook, we have adapted the dataset for anomaly detection. We removed two columns: one with the class labels (good or bad) and another which was a constant (0) for all data instances. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read the data in as a pandas dataframe and then place it in numpy array for compatibility with our existing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-30T03:31:33.136739Z",
     "start_time": "2018-12-30T03:31:33.115822Z"
    }
   },
   "outputs": [],
   "source": [
    "ion_df = pd.read_csv('ionosphere_data.csv', header=None)\n",
    "ion_data = np.array(ion_df.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-30T03:31:34.603228Z",
     "start_time": "2018-12-30T03:31:34.597654Z"
    }
   },
   "outputs": [],
   "source": [
    "ion_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-30T03:31:38.432129Z",
     "start_time": "2018-12-30T03:31:38.428074Z"
    }
   },
   "outputs": [],
   "source": [
    "print(ion_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise #1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise refers to Section 1 (linear regression model)\n",
    " \n",
    "For the ionosphere dataset, there isn't a natural dependent variable, so we have to choose one. Create a function `linreg_anomaly_scores` that returns the anomalies scores of a linear regression model where two inputs are provided: the data and the index of the feature that is the dependent variable. We will use this function in Exercise 4 to analyze the ionosphere data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note 1:** Note that since we don't have separate training data, for simplicity you should score the anomalies on all of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note 2:** You may wish to use the following helper function. For data with _n_ features, it returns an array of length _n_ with all entries `False` except for a single `True` values at a specified index (_idx_)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-30T03:31:43.471511Z",
     "start_time": "2018-12-30T03:31:43.468040Z"
    }
   },
   "outputs": [],
   "source": [
    "def idx_to_boolean(n, idx):\n",
    "    select = np.zeros(n,dtype=np.bool)\n",
    "    select[idx] = True\n",
    "    return select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-30T03:29:44.328139Z",
     "start_time": "2018-12-30T03:29:44.318472Z"
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise #2\n",
    "\n",
    "This exercise refers to Section 2 (PCA).\n",
    "\n",
    "Create a function `pca_anomaly_scores` that  returns the anomalies scores of a PCA model where two inputs are provided: the data and the number of components. We will use this function in Exercise 4 to analyze the ionosphere data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-30T03:29:44.339255Z",
     "start_time": "2018-12-30T03:29:44.331499Z"
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise #3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise refers to Section 3 (one-class SVM).\n",
    "\n",
    "Create a function `svm_anomaly_scores` that returns the anomalies scores of a one-class SVM model where two sets inputs are provided: the data and a kernel. We will use this function in Exercise 4 to analyze the ionosphere data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-30T03:32:16.084883Z",
     "start_time": "2018-12-30T03:32:16.082532Z"
    }
   },
   "outputs": [],
   "source": [
    "# Choose the default kernel to be linear\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise #4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have three `[method]_outlier_scores` functions.  Apply each of them to the ionosphere dataset.  What are the top five most anomalous examples using each technique?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-30T03:32:37.076091Z",
     "start_time": "2018-12-30T03:32:37.063929Z"
    }
   },
   "outputs": [],
   "source": [
    "methods = [linreg_anomaly_scores,\n",
    "           pca_anomaly_scores,\n",
    "           svm_anomaly_scores]\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three different methods, three different answers. Therefore more exploration is needed, which we encourage the reader to pursue. For example, choosing different target feature for the linear regression; increase the number of components in PCA; or change the kernel for one-class SVM. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last suggestion can be implemented readily for the RBF kernel, since we used it in Section 3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-30T03:29:44.407347Z",
     "start_time": "2018-12-30T03:29:44.389964Z"
    }
   },
   "outputs": [],
   "source": [
    "print(sorted(do_svm_anomaly_scores(ion_data).argsort()[-5:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as for PCA with one component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "In this assignment you should have learned: \n",
    "\n",
    "1. How to apply linear regression models\n",
    "2. How to apply PCA\n",
    "3. How to apply one-class SVMs\n",
    "\n",
    "Congratulations! This concludes the lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Intel Anomaly Env",
   "language": "python",
   "name": "intel_anomaly"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
