{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2019 Intel Corporation\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In Lesson 4, _Anomaly detection: Proximity-based methods_, we looked at ways of flagging anomalies by using a variety of distance-based methods. In particular, we looked at flagging points using\n",
    "- distance based methods: anomalies are points \"far\" from other points\n",
    "- clustering methods: anomalies are points that are at the edges of their cluster / not assigned to a cluster\n",
    "- density-based methods: anomalies are points with fewer neighbors than typical points in the same region.\n",
    "\n",
    "# Learning outcomes\n",
    "\n",
    "You should come away from this notebook with the ability to\n",
    "1. Make a kNN anomaly detector (distance-based) \n",
    "2. Make a k-means anomaly detector (cluster-based) \n",
    "3. Use the Local Outlier Factor to classify anomalies \n",
    "\n",
    "\n",
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-15T04:53:47.664825Z",
     "start_time": "2018-12-15T04:53:46.261088Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn.datasets as sk_data\n",
    "import sklearn.neighbors as neighbors\n",
    "import sys\n",
    "\n",
    "np.set_printoptions(suppress=True, precision=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python and library versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-15T04:53:47.673857Z",
     "start_time": "2018-12-15T04:53:47.667529Z"
    }
   },
   "outputs": [],
   "source": [
    "packages = [np]\n",
    "\n",
    "msg = f\"\"\"\n",
    "Python Version: {sys.version}\n",
    "\n",
    "library .      version\n",
    "-------        -------\"\"\"\n",
    "print(msg)\n",
    "\n",
    "for package in packages:\n",
    "    print(f\"{package.__name__:11}    {package.__version__:>7}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Distance-Based Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest Nearest Neighbor (NN) method is to take the distance to the k-nearest neighbor as the anomaly score.  The next variation is to use the average of the distances to the top-k neighbors as the score.  `sklearn` provides `neighbors.NearestNeighbors` to compute the nearest neighbors from a dataset.  After fitting, we can use `kneighbors()` to return the distances and indices of the top-k neighbors.  We can use `kneighbors_graph()` to return the entire connectivity graph (as an array with 1 indicating a link) for the dataset.  The graph is a sparse array, but you can operate on it the same as a normal (dense) array.  If you must have a normal array, you can use `todense()` to make it dense.\n",
    "\n",
    "The idea for the kNN models is \n",
    "- look at the $k$ neighbors of each point\n",
    "- assign a score. Roughly, a low score means the point's $k$ neighbors are close.\n",
    "- there are multiple ways of determining the score:\n",
    "    - maximum of the distances (i.e. distance to $k$th nearest neighbor)\n",
    "    - average the $k$ distances\n",
    "    - harmonic mean of the $k$ distances.\n",
    "\n",
    "Use the score to determine if something is an anomaly by using either a _threshold_ or _ranking_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by creating a simple dataset with one anomaly and then plot the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-15T04:53:48.056992Z",
     "start_time": "2018-12-15T04:53:47.676955Z"
    }
   },
   "outputs": [],
   "source": [
    "X = np.array([[0.9, 1], [0, 1], [1, 0], [0, 0], [0.5, 0.5], [0.2, 0.5], [1, 0.5], [2, 2]])\n",
    "\n",
    "plt.figure(dpi=150)\n",
    "plt.title('Upper right point looks anomalous')\n",
    "plt.xlabel('$X_1$')\n",
    "plt.ylabel('$X_2$')\n",
    "plt.plot(X[:, 0], X[:, 1], 'o');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a function to visualize a query point and its neighbors more explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-15T04:53:48.326872Z",
     "start_time": "2018-12-15T04:53:48.059729Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_point_and_k_neighbors(X, highlight_index, n_neighbors=2):\n",
    "    \"Plots the points in X, and shows the n_neighbors of the highlight_index-th point\"\n",
    "    nn = neighbors.NearestNeighbors(n_neighbors=n_neighbors).fit(X)\n",
    "    dist, index = nn.kneighbors()\n",
    "    \n",
    "    src_pt = X[highlight_index, :]\n",
    "    \n",
    "    plt.figure(dpi=150)\n",
    "    # draw lines first, so points go on top\n",
    "    for dest_index in index[highlight_index]:\n",
    "        dest_pt = X[dest_index, :]\n",
    "        plt.plot(*list(zip(src_pt, dest_pt)), 'k--')\n",
    "    plt.plot(X[:, 0], X[:, 1], 'o', label='Not k-neighbors', alpha=0.3)\n",
    "    plt.plot(*src_pt, 'o', label='The query point')\n",
    "    plt.plot(X[index[highlight_index], 0], X[index[highlight_index], 1], 'o', label='k-neighbors')\n",
    "    plt.xlabel('$X_1$')\n",
    "    plt.ylabel('$X_2$')\n",
    "    plt.legend()\n",
    "    \n",
    "# Example of usage\n",
    "plot_point_and_k_neighbors(X, 0, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see the 4 points close to the query point (index of 0 in our data) are relatively close. Let's look at the anomalous point in the upper right (this point has an index of 7):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-15T04:53:48.581247Z",
     "start_time": "2018-12-15T04:53:48.331016Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_point_and_k_neighbors(X, 7, n_neighbors=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see in this case all of the 4-nearest neighbors to our point are far away, so it seems reasonably unambiguous to call this point an anomaly.\n",
    "\n",
    "Let's add one more point next to the anomaly, and see how choosing \"k\" and the scoring function affects things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-15T04:53:48.891711Z",
     "start_time": "2018-12-15T04:53:48.584415Z"
    }
   },
   "outputs": [],
   "source": [
    "# Toy dataset with two adjacent anomalies\n",
    "X2 = np.concatenate([X, [[1.9, 2.0]]])\n",
    "\n",
    "# Look at nearest neighbor (k=1)\n",
    "plot_point_and_k_neighbors(X2, 7, n_neighbors=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation 1: dependence on $k$\n",
    "\n",
    "The distance between point 7 and the new point is actually quite smallâ€”smaller than the distances between any pair of the points in the lower left. If $k=1$, the two points in the upper right would be the _last_ ones to classified as an anomaly (i.e. they would have the lowest score).\n",
    "\n",
    "### Observation 2: how to weight distances?\n",
    "\n",
    "Let's look at the same data set (i.e. with the additional point) but now look at $k=3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-15T04:53:49.135400Z",
     "start_time": "2018-12-15T04:53:48.894188Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_point_and_k_neighbors(X2, 7, n_neighbors=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see one of the 3 nearest neighbors is very close, but the other two are far away. We need some way of combining these 3 distances into a single score. The three commonly used methods are\n",
    "1. Use the longest distance\n",
    "2. Use the (arithmetic) mean distance\n",
    "3. Use the harmonic mean.\n",
    "\n",
    "We will implement the longest distance, and assign the arithmetic mean distance and harmonic mean distance as exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the longest distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-15T04:53:49.145917Z",
     "start_time": "2018-12-15T04:53:49.138019Z"
    }
   },
   "outputs": [],
   "source": [
    "def do_nn_outlier_scores(obs, n_neighbors=1):\n",
    "    \"\"\"\n",
    "    Gives the score of a point as the distance from point to its k-th nearest neighbor.\n",
    "    Larger score means more likely to be an outlier\n",
    "    \"\"\"\n",
    "    nn = neighbors.NearestNeighbors(n_neighbors=n_neighbors)\n",
    "    nn.fit(obs)\n",
    "    dists, idx = nn.kneighbors()\n",
    "    scores = dists[:,-1]\n",
    "    return scores\n",
    "\n",
    "# Test \n",
    "do_nn_outlier_scores(X2, n_neighbors=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make the output a little easier to understand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-15T04:53:49.157021Z",
     "start_time": "2018-12-15T04:53:49.147815Z"
    }
   },
   "outputs": [],
   "source": [
    "def print_ranked_scores(obs, scores):\n",
    "    scores_and_obs = sorted(zip(scores, obs), key=lambda t: t[0], reverse=True)\n",
    "    print('Rank  Point\\t\\tScore')\n",
    "    print('------------------------------')\n",
    "    for index, score_ob in enumerate(scores_and_obs):\n",
    "        score, point = score_ob\n",
    "        print(f'{index+1:3d}.  {point}\\t\\t{score:6.4f}')\n",
    "\n",
    "# Look at the outliers using 3 neighbors\n",
    "print_ranked_scores(X2, do_nn_outlier_scores(X2, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now look at the anomalies with k=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-15T04:53:49.166908Z",
     "start_time": "2018-12-15T04:53:49.159872Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "print_ranked_scores(X2, do_nn_outlier_scores(X2, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TAKEAWAY:** The number of neighbors $k$ used in kNN drastically changes the results. Usually requires some domain expertise to know the value of $k$ or range of values for $k$ to be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kNN in reverse: outlier detection using in-degree (ODIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Another distance-based variation, called ODIN (outlier detection using in-degree), uses the same neighbors in a different way.  As discussed in the lesson, this algorithm asks the question, \"who am I the nearest neighbor of?\".  So, an example that serves as a NN of many others -- with many incoming links in the NN graph -- has a *low* anomaly score.  Examples with a small in-degree (= # of incoming links), are more likely to be anomalies. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an implementation of ODIN. Note that we convert the scores from the indegree value (low for an anomaly) to a zero-to-one scale where higher values are more of anomaly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-15T04:53:49.203005Z",
     "start_time": "2018-12-15T04:53:49.195384Z"
    }
   },
   "outputs": [],
   "source": [
    "def do_odin_outlier_scores(obs, n_neighbors=3):\n",
    "    nn = neighbors.NearestNeighbors(n_neighbors=n_neighbors).fit(obs)\n",
    "    graph = nn.kneighbors_graph()\n",
    "    indegree = graph.sum(axis=0)  # sparse matrix\n",
    "\n",
    "    # smaller indegree means more of an anomaly  \n",
    "    # simple conversion to [0,1] so larger score is more of anomaly\n",
    "    scores = (indegree.max() - indegree) / indegree.max()\n",
    "    return np.array(scores)[0]  # convert to array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score and rank the points for the _X2_ dataset (two adjacent anomalies)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-15T04:53:49.217792Z",
     "start_time": "2018-12-15T04:53:49.206104Z"
    }
   },
   "outputs": [],
   "source": [
    "scores_odin= do_odin_outlier_scores(X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_ranked_scores(X2, scores_odin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two anomalies (2, 2) and (1.9, 2) have large scores, but so do the points (0, 1) and (0, 1). Also, the result is different from we found with k-means results. As we discussed in the lecture, the method you use matters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Cluster-Based Method (k-Means)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering via k-means can be used as an anomaly detection method by:\n",
    "\n",
    "  1. generating clusters, \n",
    "  2. finding the cluster of each example, \n",
    "  3. using the distance from the example to its cluster's center as a score\n",
    "  \n",
    "These pieces are available using `cluster.KMeans` which after fitting and predicting to get the clusters -- both where the clusters are and what cluster an example belongs to -- provides a `cluster_centers_` attribute.  Then we can use numpy's `numpy.linalg.norm` (often imported as `import numpy.linalg as nla`) to compute distances for us.\n",
    "\n",
    "We will start by generating some data, then showing what the clusters are, and finally scoring the anomalies. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data generation and view**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-15T04:53:49.388605Z",
     "start_time": "2018-12-15T04:53:49.220209Z"
    }
   },
   "outputs": [],
   "source": [
    "blobs_X, cluster_labels = sk_data.make_blobs(centers=[[0,0], [10,10], [10,0]])\n",
    "anomalies, _ = sk_data.make_blobs(centers=[[5,5]], n_samples=5, cluster_std=3, random_state=42)\n",
    "\n",
    "data = np.concatenate([blobs_X, anomalies])\n",
    "cluster_labels = np.concatenate([cluster_labels, [-1]*len(anomalies)])\n",
    "\n",
    "# Display the data before clustering\n",
    "plt.figure(dpi=110)\n",
    "plt.plot(data[:, 0], data[:, 1], 'o');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-15T04:05:10.033651Z",
     "start_time": "2018-12-15T04:05:09.880698Z"
    }
   },
   "source": [
    "**Clustering and show cluster assignment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-15T04:53:49.811522Z",
     "start_time": "2018-12-15T04:53:49.394253Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "km = KMeans(n_clusters=3).fit(data)\n",
    "\n",
    "plt.figure(dpi=120)\n",
    "\n",
    "for label in range(3):\n",
    "    mask = (km.labels_ == label)\n",
    "    plt.plot(data[mask, 0], data[mask, 1], 'o', label=f'Cluster {label}')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should run the clustering several times and note that the assignment of some points changes due to the different initial conditions (random choice of starting centroids)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For each point, find the distance from the point to its centroid**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-15T04:53:49.822356Z",
     "start_time": "2018-12-15T04:53:49.816242Z"
    }
   },
   "outputs": [],
   "source": [
    "centers = km.cluster_centers_[km.labels_]\n",
    "# show the centers for the first 10 points\n",
    "centers[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-15T04:53:49.838567Z",
     "start_time": "2018-12-15T04:53:49.832620Z"
    }
   },
   "outputs": [],
   "source": [
    "#Get the distances to the centers as use these as the scores\n",
    "score = np.linalg.norm(data - centers, axis=1)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the 5 points with the highest scores? We will call those the anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-15T04:53:49.849334Z",
     "start_time": "2018-12-15T04:53:49.843942Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "np.argsort(score)[::-1][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show these results in a plot that includes the cluster centers (labeled with an 'x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-15T04:53:50.152128Z",
     "start_time": "2018-12-15T04:53:49.851153Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "km = KMeans(n_clusters=3).fit(data)\n",
    "\n",
    "anomaly_idx = np.argsort(score)[::-1][:5]\n",
    "anomaly_mask = np.zeros(len(data))\n",
    "anomaly_mask[anomaly_idx] = 1\n",
    "\n",
    "plt.figure(dpi=120)\n",
    "colors = ['blue', 'orange', 'green']\n",
    "\n",
    "for label, color in enumerate(colors):\n",
    "    mask = (km.labels_ == label) & (anomaly_mask == 0)\n",
    "    plt.plot(data[mask, 0], data[mask, 1], marker='o', linestyle='none',\n",
    "             color=color, label=f'Cluster {label}')\n",
    "    plt.plot(*km.cluster_centers_[label], marker='x', color='k')\n",
    "\n",
    "plt.plot(data[anomaly_idx, 0], data[anomaly_idx, 1], marker='o', linestyle='none',\n",
    "         color='k', label='Anomaly')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you rerun the clustering enough times, you will find that the three central points are always labeled as anomalies. The other two anomalies vary depending on how the points are clustered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: even bigger changes can be expected if the number of clusters is varied. Below we illustrate the effect of varying the number of clusters from 1 to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-15T04:53:51.193270Z",
     "start_time": "2018-12-15T04:53:50.474799Z"
    }
   },
   "outputs": [],
   "source": [
    "# Show clustering for user-specified number of clusters\n",
    "import matplotlib.pyplot as plt\n",
    "fig, axes = plt.subplots(1,5, figsize=(15,3))\n",
    "for ax, n_clusters in zip(axes, [1,2,3,4,5]):\n",
    "    clust = KMeans(n_clusters=n_clusters)\n",
    "    obs_to_clusters = clust.fit_predict(data) \n",
    "    ax.scatter(*data.T, c=obs_to_clusters)\n",
    "    ax.set_title(\"n_clusters={}\".format(n_clusters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Density-Based Methods (Local Outlier Factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The local outlier factor (LOF) is `sklearn` has a version available for us to use. Note the the LOF returned by `sklearn` is the negative of the value we defined in the lecture.\n",
    "\n",
    "Apply LOF to the collection of points ('X2') we created in section 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.neighbors as neighbors\n",
    "lof = neighbors.LocalOutlierFactor(n_neighbors=3, contamination='auto')\n",
    "lof.fit(X2)\n",
    "sk_lof = -lof.negative_outlier_factor_\n",
    "print_ranked_scores(X2, sk_lof)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See values both above and below 1. As discussed in the lecture, we expect the values significantly above 1 to be anomalies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare with the kNN results from before. Note we use the same value of k (=3) for both algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_ranked_scores(X2, do_nn_outlier_scores(X2, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two anomalous points are both found either way, but there are differences between the results for the other points. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For completeness, we provide below an explicit implementation of LOF that follows the outline given in the lecture. To check that our algorithm is reasonable, we compare the results for its anomaly score with those of the `sklearn` version for the cluster blobs ('data')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-15T04:53:51.199608Z",
     "start_time": "2018-12-15T04:53:51.194728Z"
    }
   },
   "outputs": [],
   "source": [
    "def lof_method(obs, n_neighbors=2):\n",
    "    neigh = neighbors.NearestNeighbors(n_neighbors=2).fit(obs)\n",
    "    \n",
    "    #Return indices of and distances to the neighbors of each point\n",
    "    topk_dist, my_kneigh = neigh.kneighbors()\n",
    "    \n",
    "    # Create list of distances of furthest (kth) neighbor \n",
    "    k_dist = topk_dist[:,-1]\n",
    " \n",
    "    # Reachability distance: maximum of true distance between query neighbor and query point\n",
    "    # and distance to kth nearest neighbor of query neighbor\n",
    "    reach = np.maximum(topk_dist, k_dist[my_kneigh])\n",
    "    \n",
    "    # Local reacability density is reciprocal of average reachability distance\n",
    "    lrd = 1.0 / np.mean(reach, axis=1)\n",
    "    \n",
    "    # Local outlier factor is given by\n",
    "    # average local density of neighbors / local density of query point\n",
    "    lrd_ratios = lrd[my_kneigh] / lrd[:, np.newaxis]\n",
    "    lof = np.mean(lrd_ratios, axis=1)\n",
    "    \n",
    "    return lof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose k=2\n",
    "our_lof = lof_method(data, 2)\n",
    "our_lof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lof2 = neighbors.LocalOutlierFactor(n_neighbors=2, contamination='auto')\n",
    "lof2.fit(data)\n",
    "sk_lof2 = -lof2.negative_outlier_factor_\n",
    "print('Same as sklearn?', np.allclose(our_lof, sk_lof2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise #1\n",
    "\n",
    "This exercise refers to the distance-based methods (Section 1).\n",
    "\n",
    "A. Create a function `do_nn_avg_scores(obs, n_neighbors=1)` that computes outlier scores using arithmetic mean distance from the point to each of the `n_neighbors` nearest neighbors as the score. \n",
    "\n",
    "B. Do the same thing as in part (A) to create `do_nn_harm_scores(obs, n_neighbors=1)`, where you use the harmonic mean instead of the mean. The harmonic mean of $n$ points is defined as\n",
    "\n",
    "$$\\text{harmonic}(X_1, X_2, \\ldots, X_n) = \\frac{n}{(1/x_1) + (1/x_2) + \\ldots + (1/x_n)} = \\frac{\\left(\\prod X_i\\right)^{1/n}}{\\bar{X}} = \\frac{(X_1X_2\\cdot X_n)^{1/n}}{\\bar{X}}$$\n",
    "\n",
    "   Note that `scipy.stats` contains a `hmean` function you can use.\n",
    "  \n",
    "Try each of these functions on our toy dataset, $X2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise #2\n",
    "\n",
    "This exercise refers to cluster-based methods (k-means; Section 2).\n",
    "\n",
    "A. Combine the code in the individual cells to create a function `do_cluster_outlier_scores(obs, n_clusters=3)` that computes outlier scores using the cluster method. Check that you find the same top five anomalies as before.\n",
    "\n",
    "B. Combine the code in the individual cells to create a function `plot_clusters_and_outliers(obs, n_clusters=3, n_anomalies=5)` that generates a plot like the one above. Do you get the same results as before?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise #3\n",
    "\n",
    "This exercise refers to density-based methods (Local Outlier Factor; Section 3).\n",
    "\n",
    "\n",
    "A. Create a function `do_lof_outlier_scores(obs, n_neighbors=3)` that computes outlier scores using the LOF method.  Recall that the values returned by sklearn's implementation are negatives of what we want.\n",
    "\n",
    "B. Apply your function to our toy dataset `data` to get the top five anomalies.\n",
    "\n",
    "C. How sensitive is the LOF method to `n_neighbors`?  Trying varying `n_neighbors` on the `data` dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution #3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "In this assignment you should have learned: \n",
    "\n",
    "1. How to make a kNN anomaly detector (distance based) \n",
    "2. How to make a k-means anomaly detector (cluster based) \n",
    "3. To use the Local Outlier Factor to classify anomalies \n",
    "\n",
    "Congratulations! This concludes the lesson."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Intel Anomaly Env",
   "language": "python",
   "name": "intel_anomaly"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
