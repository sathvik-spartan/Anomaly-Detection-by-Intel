{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2019 Intel Corporation\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In Lesson 5 of *Anomaly Detection*, we learned that high dimensional data introduces additional complications to anomaly detection (\"the curse of dimensionality\"). We also discussed two approaches to anomaly detection in high dimensions: the subspace method with feature bagging and isolation forests.\n",
    "\n",
    "Here will use both the subspace method and isolation forests to detect anomalies in high dimensional simulated datasets.\n",
    "\n",
    "\n",
    "# Learning Outcomes\n",
    "\n",
    "You should walk away from this Python tutorial with:\n",
    "1. An understanding of the challenges of anomaly detection with high dimensional data\n",
    "2. Some practical experience with the subspace method and feature bagging\n",
    "3. Some practical experience with isolation forests \n",
    "\n",
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "import sys\n",
    "import datetime\n",
    "import scipy\n",
    "import scipy.stats as ss\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.ensemble import IsolationForest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python and library versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "versions = ( (\"matplotlib\", matplotlib.__version__),\n",
    "            (\"numpy\", np.__version__),\n",
    "            (\"pandas\", pd.__version__),\n",
    "            (\"scipy\", scipy.__version__))\n",
    "\n",
    "print(sys.version, \"\\n\")\n",
    "print(\"library\" + \" \" * 4 + \"version\")\n",
    "print(\"-\" * 18)\n",
    "\n",
    "for tup1, tup2 in versions:\n",
    "    print(\"{:11} {}\".format(tup1, tup2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: The subspace method with feature bagging\n",
    "\n",
    "In this section, we are going to use simulated data to illustrate the subspace method with feature bagging. \n",
    "\n",
    "Our approach will follow an early publication in anomaly detection using the subspace method with feature bagging (A. Lazarevic and V. Kumar. Feature Bagging for Outlier Detection. ACM KDD Conference, 2005). It is \n",
    "available here: http://neuro.bstu.by/ai/To-dom/My_research/Papers-0/For-research/D-mining/Anomaly-D/KDD-cup-99/p157.pdf\n",
    "\n",
    "We will proceed as follows:\n",
    "\n",
    "1. Create the simulated data\n",
    "2. Perform anomaly detection with the subspace method/feature bagging\n",
    "3. Score the results\n",
    "4. Analyze the output using a confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by creating the data, which will be a combination of normal points and anomalies for a total of 5100 points. The 5000 normal points are generated from a 5D Gaussian distribution where we specify the mean in each dimension as well as the covariance matrix.  The anomalies are 100 points that are far away from the normal points in the same 5D space (the relevant features). To explore the effects of feature bagging, an additional 5D of noise (irrelevant features) are added to all data points.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will generate the 5000 normal data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(20) # include a seed for reproducibility\n",
    "\n",
    "# generate the normal data using number of points, \n",
    "# number of relevant features, mean and standard deviation\n",
    "norm_pts = 5000\n",
    "norm_dim = 5\n",
    "norm_mean = 0.0\n",
    "norm_stdev = 0.5\n",
    "norm_data = np.random.normal(norm_mean, norm_stdev, \n",
    "                             (norm_pts, norm_dim))\n",
    "\n",
    "# Note:\n",
    "#\n",
    "# Do not confuse the two uses of 'normal'\n",
    "# The 'normal' in np.random.multivariate_normal refers \n",
    "# to a Gaussian distribution and is not related in general \n",
    "# to normal vs. anomaly \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we generate the anomalous data. We will do this in two steps:\n",
    "\n",
    "A. Generate a Gaussian distrubution of points\n",
    "\n",
    "B. Transform this distribution into a ring around the normal points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Gaussian distribution for preliminary anomaly data\n",
    "# using number of points, number of relevant features, \n",
    "# mean and standard deviation\n",
    "anom_pts = 100\n",
    "anom_dim = 5\n",
    "anom_mean = 0.0\n",
    "anom_stdev = 0.5\n",
    "anom_data_prelim = np.random.normal(anom_mean, anom_stdev, \n",
    "                             (anom_pts, anom_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform into ring distribution for final anomaly data\n",
    "def transform_to_ring(data, radius, spread):\n",
    "    \"\"\"\n",
    "    Transforms the data provided into a ring distribution.\n",
    "    ---\n",
    "    Inputs: data (the data to be transformed, np.array),\n",
    "    radius (radius of ring, float)\n",
    "    spread (spread of data about radius, float)\n",
    "    Outputs: transformed_data (np.array)\n",
    "    \"\"\"\n",
    "    transformed_data_list = []\n",
    "    for item in data:\n",
    "        z = np.array(item) # in case data is provided as list\n",
    "        transformed_data_list.append(z*spread \n",
    "                                     + radius*z / np.linalg.norm(z))\n",
    "        transformed_data = np.array(transformed_data_list)\n",
    "    return transformed_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anom_data = transform_to_ring(anom_data_prelim, 2.0, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the data and color code normal vs. anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,4))\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "ax1.scatter(norm_data[:,0], norm_data[:,1], s=30, c='b', marker=\"o\", \n",
    "            label='normal')\n",
    "ax1.scatter(anom_data[:,0], anom_data[:,1], s=30, c='r', marker=\"o\", \n",
    "            label='anomaly')\n",
    "plt.legend(loc='best');\n",
    "plt.xlabel('x', fontsize=12)\n",
    "plt.ylabel('y', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are plotting a 5D distribution in 2D, it is hard to see the ring nature of the anomaly data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: How could we quickly check that our ring transformation is working?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: Look at the anomalies in 2D. Set the parameter *anom_dim* to 2 and explore what happens when you change *radius* and *spread*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the data into one array for later use\n",
    "combined_data = np.concatenate((norm_data, anom_data), axis=0)\n",
    "combined_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create noise for normal data (1) and anomalies (2)\n",
    "# using number of irrelevant features, \n",
    "# mean and standard deviation\n",
    "noise_dim = 5\n",
    "noise_mean = 0\n",
    "noise_stdev = 4\n",
    "noise_norm = np.random.normal(noise_mean, noise_stdev, \n",
    "                                    (norm_pts, noise_dim))\n",
    "noise_anom = np.random.normal(noise_mean, noise_stdev, \n",
    "                                    (anom_pts, noise_dim ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add noise to normal data and anomalies \n",
    "# to create full data (all features)\n",
    "norm_data_full = np.concatenate((norm_data, noise_norm), axis=1)\n",
    "anom_data_full = np.concatenate((anom_data, noise_anom), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate normal data and anomalies for later use\n",
    "all_data = np.concatenate((norm_data_full, anom_data_full), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anomaly detection with feature bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A reminder of how feature bagging works in $d$ dimensions:\n",
    "    \n",
    "1. Select an integer $m$ at random from $\\lfloor{d/2}\\rfloor$ to $d-1$\n",
    "2. For each iteration, select $m$ features at random without replacement from the full dataset producing an $m$-dimensional subset \n",
    "3. Apply the anomaly detection algorithm to the subset to score each data point\n",
    "4. Combine scores from the different iterations to get an overall result for each point\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by defining a function to implement steps 1 and 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_bagging(data):\n",
    "    \"\"\"\n",
    "    Selects subspace data using feature bagging.\n",
    "   \n",
    "    Args: \n",
    "        data: the full data to be sampled (np.array)\n",
    "        \n",
    "    Returns: \n",
    "        subspace_data: the subspace data (np.array)\n",
    "    \"\"\"\n",
    "    # Find the size of the full dimensional space\n",
    "    full_dim = data.shape[1]\n",
    "    # select size of subspace\n",
    "    size_subspace = np.random.randint(full_dim // 2, full_dim - 1)\n",
    "    # select features without replacement \n",
    "    subspace_index = np.random.choice(full_dim, size_subspace, \n",
    "                                      replace=False)\n",
    "    # select subspace data \n",
    "    # Originally, features are data columns\n",
    "    # Transponse to select features as rows (easier)\n",
    "    # and then transponse back to restore features as columns\n",
    "    subspace_data = (data.T[sorted(subspace_index),:]).T\n",
    "    return subspace_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now carry out anomaly detection on subspace data (step 3). We will use the local outlier factor (LOF) approach discussed in Lesson 4. LOF was also used by Lazarevic and Kumar because it returns appropriately normalized scores that account for feature subspaces of different dimensions. \n",
    "\n",
    "To combine the scores from each iteration (step 4), we will use the cumulative-sum approach (see Lesson 05 lecture)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anomaly_subspace_method(data, repeat):\n",
    "    \"\"\"\n",
    "    Carries out anomaly detection using feature bagging.\n",
    "    Uses Local Outlier Factor (LOF) for anomaly detection.\n",
    "    Uses cumulative-sum approach to combine anomaly detection scores. \n",
    "    from each iteration\n",
    "    \n",
    "    Args: \n",
    "        data: the full data to be sampled (np.array)\n",
    "        repeat: number of iterations for feature bagging (int)\n",
    "    Returns:\n",
    "        final_scores: the anomaly score of each point in the data (list)\n",
    "    \"\"\"\n",
    "    final_scores = np.zeros(len(data))\n",
    "    for i in range(repeat):\n",
    "        subspace_data = feature_bagging(data)\n",
    "        clf = LocalOutlierFactor(n_neighbors=20, metric='manhattan')\n",
    "        y_pred = clf.fit_predict(subspace_data)\n",
    "        X_scores = clf.negative_outlier_factor_\n",
    "        final_scores += X_scores\n",
    "    return final_scores\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**—we use the *negative_outlier_factor_* scoring convention of *sklearn*:\n",
    "\n",
    "Inliers tend to have a *negative_outlier_factor_* close to -1.\n",
    "<br>outliers tend to have a large negative number.\n",
    "\n",
    "See also Lesson 04 and http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to analyze the data. We'll repeat the feature bagging 50 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_scores = anomaly_subspace_method(all_data, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_scores.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that the first 5000 points are normal data and the next 100 are anomalies. Therefore we expect that the scores at the beginning of *cumulative_scores* should be less negative than those at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cumulative_scores[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cumulative_scores[5080:5100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks reasonable, but with so many data points, we must go beyond simple inspections and do some statistical analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the predicted anomalies will have the lowest scores, we sort the scores from low to high and return the index of each corresponding data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking_by_index = np.argsort(cumulative_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, let's do a quick check. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ranking_by_index[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the 20 points with the lowest scores have indices that are 5000 or greater. Encouraging!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a confusion matrix, we will create two arrays: one with the true labels for each point and the other with the predicted labels. We will use a 1 for the anomalies and a 0 for the normal points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create labels based on known data\n",
    "# We know that the first *norm_pts* are normal data (0)\n",
    "# and the next *anom_pts* are anomalies (1)\n",
    "label_true = [0]*norm_pts\n",
    "for i in range(anom_pts):\n",
    "        label_true.append(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the predicted labels using *ranking_by_index*. Since we know that we have 100 anomalies , we will label the first 100 lowest scores as anomalies and the rest as normal data. (To be more general, we will use the parameter *anom_pts* instead of a number)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_pred = [0]*len(all_data)\n",
    "for index, item in enumerate(ranking_by_index):\n",
    "    label_pred[item] = 1\n",
    "    if index == anom_pts:\n",
    "        # We stop once we have label *anom_pts* points \n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our labels, we can construct a confusion matrix. One way to do so is with pandas dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_true_series = pd.Series(label_true, name='True')\n",
    "label_pred_series = pd.Series(label_pred, name='Predicted')\n",
    "df_confusion_matrix = pd.crosstab(label_true_series, label_pred_series)\n",
    "df_confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We correctly found 86 out of 100 anomalies and also found 4985 out of 5000 normal data points. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can use scikit-learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See Imports section above\n",
    "confusion_matrix(label_true, label_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The advantage of scikit-learn is that is has many useful functions to evaluate different statistical measures of performance. For example, we can calculate the accuracy, which is the proportion of true results (both true positives and true negatives) out of the total number of points analyzed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(label_true, label_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For additional statistical measures, see http://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we have created superb anomaly detector, but before we celebrate, we should remember that we had significant help: we included the number of anomalies (but not their identities) in the scoring procedure. If we didn't know how many anomalies we had, the anomaly detection would become more challenging. \n",
    "\n",
    "(And see Lesson 06 for a discussion of the limitations of accuracy as a metric for anomaly detection algorithms.)\n",
    "\n",
    "Also, there were several parameters we had to choose: *repeat* for the number of times to carry or feature bagging;  *n_neighbors* and *metric* for the LOF evaluation. There are rules of thumb for chooing these paramters, but often exploratory work is need to find the appropriate values for the problem at hand. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Isolation forests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we are going to examine isolation forests using the same simulated data we analyzed with the subspace method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will proceed just as we did with the subspace method:\n",
    "\n",
    "1. Create the simulated data—**done already**\n",
    "2. Perform anomaly detection with the isolation forest algorithm\n",
    "3. Score the results\n",
    "4. Analyze the output using a confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anomaly detection with isolation forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A reminder of how isolation forest works: \n",
    "\n",
    "\n",
    "<li> It is an ensemble method: an isolation forest is a combination of isolation trees.</li>\n",
    "\n",
    "<li> To create an isolation tree, a feature is chosen at random and the data is repeated divided with axis-parallel cuts at randomly chosen positions.</li>\n",
    "\n",
    "<li> This process is repeated with the goal to divide the data into nodes with fewer and fewer points until a singleton node containing one point is reached (the “leaf” of the tree).</li>\n",
    "\n",
    "<li> We expect the tree branches containing anomalies to be less deep than those for normal points, because anomalies are located in sparse regions.</li>\n",
    "\n",
    "<li>Therefore, the depth of the branch can be used to score the point in the leaf. </li>\n",
    "    \n",
    "<li>This process is repeated for a predetermined number of trees to calculate a mean anomaly score.</li>  \n",
    "\n",
    "<li>This final score is then compared with a threshold to label the point as a normal point or an anomaly.</li>\n",
    "\n",
    "\n",
    "More details are available in the publications of the original authors of this technique (F. L. Kai, M. Ting and Z.-H. Zhou):\n",
    "https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/tkdd11.pdf\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the sklearn implementation of the isolation forest, which includes the detection algorithm as well as the scoring procedure (steps 2 and 3 in the process outlined in the previous subsection):\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html\n",
    "\n",
    "This implementation uses a different scoring convention than the original publication by Kai, Ting and Zhou. In the sklearn version, negative scores represent anomalies, positive scores represent normal data. (For a given point, the more negative the score, the more of an anomaly it is.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several key parameters for IsolationForest, the sklearn algorithm. If a training dataset is available, this dataset can be used to tune these parameters. In our case, we don't have a training dataset, so we will use reasonable (default) values as discussed below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's create our forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isolation_forest = IsolationForest(n_estimators=100, max_samples=256, \n",
    "                                   contamination=0.01961, max_features=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters explained:\n",
    "    \n",
    "1. *n_estimators*: the number of isolation trees in the forest. The default value is 100.\n",
    "\n",
    "2. *max_samples*: this is the maximum number of samples drawn to construct each tree. Sklearn offers several choices for this parameter. Kai, Ting and Zhou find that 256 is \"generally is enough to\n",
    "perform anomaly detection across a wide range of data.\"\n",
    "\n",
    "3. *max_features*: the number of features drawn to construct each tree. Here we use the full dimensionality of our dataset (10). Using a smaller value would introduce feature bagging into the tree.\n",
    "\n",
    "4. *contamination*: the fraction of points that are anomalies. Just as we did for the subspace method, we will use this number when labeling the anomalies. The number given is  (*anom_pts*)/(*anom_pts* + *norm_pts*) = 100/5100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've created the forest, we can fit our data and score the points. To do this properly, we should construct the tree with a training dataset and then apply this tree to the test data. You will do so in Exercise 2 below. \n",
    "\n",
    "For now, we're going to take a simplified approach and fit the tree and score the points using the same data. This is a cavalier approach to isolation forest that can lead to biased results, but it makes for a fairer comparison with the subspace method (where there is no train/test  split)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isolation_forest.fit(all_data)\n",
    "anomaly_score = isolation_forest.decision_function(all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as with the subspace method, we know that the first 5000 points are normal data and the next 100 are anomalies. Therefore, we expect that the scores at the beginning of *anomaly_scores* should be positive and those at the end negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(anomaly_score[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(anomaly_score[5080:5100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see some positive scores at the end, but it would a lot to ask for our isolation forest to be perfect. What does *IsolationForest* predict the last 100 points to be?\n",
    "\n",
    "**Note**: *IsolationForest* labels anomalies with -1 and normal points with +1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_forest = isolation_forest.predict(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pred_forest[5000:5100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are happy to see mainly -1, but we should calculate precise statistics. To make a clear comparison with our subspace method results, we will convert *pred_forest* to the labeling convention we used previously: anomalies are +1 and normal data points are 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_pred_forest = pred_forest\n",
    "# normal data label switched from 1 to 0\n",
    "label_pred_forest [label_pred_forest > 0] = 0 \n",
    "# anomaly label switched from -1 to 1\n",
    "label_pred_forest [label_pred_forest < 0] = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(label_true, label_pred_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(label_true, label_pred_forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we look at the confusion matrix, we can see that have fewer true positive results than with the subspace method. Nevertheless, it is a popular method because of its speed (low computation complexity)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise #1\n",
    "\n",
    "This exercise refers to Section 1 (subspace method with feature bagging). \n",
    "\n",
    "We are going to analyze the data without any feature bagging and check whether the claim we made in lectures is correct: feature bagging improves anomaly detection\n",
    "\n",
    "A. Modify *anomaly_subspace_method* to carry out anomaly detection on the full data without any feature bagging.\n",
    "\n",
    "B. Determine the confusion matrix.\n",
    "\n",
    "C. How do the results compare with the subspace method? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise #2\n",
    "\n",
    "This exercise refers to Section 2 (isolation forests).\n",
    "\n",
    "A. Create a training dataset. It should have the same features as *all_data* except it should different points and be one fifth the size (20 anomalies and 1000 normal points).\n",
    "\n",
    "\n",
    "B. Train the isolation forest on this dataset and then test the trained forest on *all_data*.\n",
    "\n",
    "C. Compare the results for the confusion matrix for the one you got without the test/train split. Discuss any differences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "In this assignment you should have learned: \n",
    "\n",
    "1. How to use the subspace method and feature bagging\n",
    "2. How to use isolation forests \n",
    "\n",
    "Congratulations! This concludes the lesson."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Intel Anomaly Env",
   "language": "python",
   "name": "intel_anomaly"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
