{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2019 Intel Corporation\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T22:23:05.847085Z",
     "start_time": "2018-12-21T22:23:03.886959Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "import sklearn.neighbors as neighbors\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python and library versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T22:23:05.859860Z",
     "start_time": "2018-12-21T22:23:05.850299Z"
    }
   },
   "outputs": [],
   "source": [
    "packages = [matplotlib, np, pd]\n",
    "\n",
    "msg = f\"\"\"\n",
    "Python Version: {sys.version}\n",
    "\n",
    "library .      version\n",
    "-------        -------\"\"\"\n",
    "print(msg)\n",
    "\n",
    "for package in packages:\n",
    "    print(f\"{package.__name__:11}    {package.__version__:>7}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing data\n",
    "\n",
    "We are going to look at sensor logs from the shuttle from the [UCI learning repository](https://archive.ics.uci.edu/ml/datasets/Statlog+%28Shuttle%29). The last column is the (manual) classification of the operational mode of the shuttle.\n",
    "\n",
    "There are no labels for the sensors, or a header row in the data, so we will manually construct the heading names `attr_0` through `attr_8` for the sensor data, and `class` for the operational mode's code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T22:23:05.904344Z",
     "start_time": "2018-12-21T22:23:05.862510Z"
    }
   },
   "outputs": [],
   "source": [
    "col_names = [f'attr_{n}' for n in range(9)] + ['class']\n",
    "df = pd.read_csv('shuttle.tst', delim_whitespace=True, names=col_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to simulate outlier detection by \n",
    "- making class 1 (~80%) the inliners\n",
    "- dropping the next largest class (class 4)\n",
    "- combining all other classes as outliers\n",
    "\n",
    "Before dropping the class column, let's look at the frequency of each class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T22:23:05.932786Z",
     "start_time": "2018-12-21T22:23:05.909952Z"
    }
   },
   "outputs": [],
   "source": [
    "df['class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to exchange the \"class\" attribute for the \"anomaly\" attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T22:23:06.012359Z",
     "start_time": "2018-12-21T22:23:05.940499Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df[ df['class']!=4 ]\n",
    "df['anomaly'] = (df['class'] != 1).astype(bool)\n",
    "df.drop('class', axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T22:23:06.032632Z",
     "start_time": "2018-12-21T22:23:06.014502Z"
    }
   },
   "outputs": [],
   "source": [
    "df.anomaly.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have approximately 7% of the data as anomalous.\n",
    "\n",
    "Let's evaluate the performance of the IsolationForest on this dataset (see lesson 5 for a discussion of IsolationForest). Note there is nothing special about the isolation forest (except we can skip scaling the data) -- we could use any of the anomaly detection classifiers on this same dataset, provided it implements a `decision_function` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making our anomaly detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T22:23:07.507624Z",
     "start_time": "2018-12-21T22:23:06.037098Z"
    }
   },
   "outputs": [],
   "source": [
    "frac_anomalies = df['anomaly'].sum()/df['anomaly'].size\n",
    "\n",
    "iso_forest = IsolationForest(behaviour='new', contamination=frac_anomalies, random_state=42)\n",
    "y_predict = iso_forest.fit_predict(df.drop('anomaly', axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the fraction of prediction for anomaly (`-1`) matches the `frac_anomalies` parameter exactly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T22:23:07.513831Z",
     "start_time": "2018-12-21T22:23:07.509376Z"
    }
   },
   "outputs": [],
   "source": [
    "msg = f\"\"\"\n",
    "Fraction of anomalies predicted:                 {(y_predict == -1).sum() / y_predict.size}\n",
    "Fraction of anomalies given to isolation forest: {frac_anomalies}\n",
    "\"\"\"\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access the raw scores from `iso_forest.decision_function`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T22:23:07.862264Z",
     "start_time": "2018-12-21T22:23:07.519603Z"
    }
   },
   "outputs": [],
   "source": [
    "y_scores = iso_forest.decision_function(df.drop('anomaly', axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric: Precision@n\n",
    "\n",
    "Sklearn does not have formula for precision@n built in, so we will have to write our own. We will demonstrate that they duplicate the results we expect on the toy dataset given on slides 5 and 6 of the presentation.\n",
    "\n",
    "The formula for P@n is\n",
    "\\begin{equation*}\n",
    "P@n = \\frac{\\text{# anomalies in first $n$ entries}}{n}\n",
    "\\end{equation*}\n",
    "Let's write a function for giving us P@n:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T22:23:07.871375Z",
     "start_time": "2018-12-21T22:23:07.863817Z"
    }
   },
   "outputs": [],
   "source": [
    "def precision_at_n(y_is_anomaly_true, y_scores, n_max = 0):\n",
    "    \"\"\"\n",
    "    y_scores: list of scores (lower means more likely to be an anomaly)\n",
    "    y_is_anomaly_true: boolean list, y_is_anomaly_true[i] is True \n",
    "    iff the ith point is an anomaly\n",
    "    \n",
    "    Returns an array of scores for P@n for n in 1, 2, 3, ...., n_max.\n",
    "    \n",
    "    If n_max is given as 0, ranks P@n for the entire dataset\n",
    "    \"\"\"\n",
    "    if not n_max:\n",
    "        n_max = len(y_scores)\n",
    "    \n",
    "    is_anomaly_sorted_by_score = [is_anomaly for _, \n",
    "                                  is_anomaly in sorted(zip(y_scores, y_is_anomaly_true))]\n",
    "    return [sum(is_anomaly_sorted_by_score[:n])/n for n in range(1, n_max+1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check this on the 4 row example we gave in lecture, where rows 1 and 3 were anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T22:23:07.882943Z",
     "start_time": "2018-12-21T22:23:07.873923Z"
    }
   },
   "outputs": [],
   "source": [
    "y_scores = np.array([-0.03598, -0.033510, -0.005384, 0.000330])\n",
    "y_true = np.array([1, 0, 1, 0])\n",
    "# demonstrate it on example given in lecture, where rows 1 and 3 were anomalous\n",
    "precision_at_n(y_true, y_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T22:23:07.894837Z",
     "start_time": "2018-12-21T22:23:07.886254Z"
    }
   },
   "outputs": [],
   "source": [
    "def custom_average_precision(y_is_anomaly_true, y_scores):\n",
    "    sorted_scores_and_truth = np.array([(score, is_anomaly) \n",
    "                                        for score, is_anomaly in \n",
    "                                        sorted(zip(y_scores, y_is_anomaly_true))])\n",
    "    scores, is_anomaly = sorted_scores_and_truth[:, 0], sorted_scores_and_truth[:, 1]\n",
    "    p_at_n = np.array(precision_at_n(is_anomaly, y_scores))\n",
    "    return (is_anomaly * p_at_n).sum() / is_anomaly.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T22:23:07.913688Z",
     "start_time": "2018-12-21T22:23:07.896817Z"
    }
   },
   "outputs": [],
   "source": [
    "# Apply this to the version given on slides\n",
    "custom_average_precision(y_true, y_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T00:47:16.817784Z",
     "start_time": "2018-12-10T00:47:16.811896Z"
    }
   },
   "source": [
    "**Warning:**\n",
    "\n",
    "Sklearn also provides a very similar looking `average_precision_score` in `sklearn.metrics`. The documentation for the function is provided [here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html).\n",
    "\n",
    "This function is a little odd to use, because it is primarily used for classification problems (where the score is interpreted as a _higher_ score means _more_ likely to be the positive class). Most of our `decision_functions` output have anomalies default to class `-1`, and a _lower_ (i.e. more negative score) means the point is _more_ likely to be an anomaly.\n",
    "\n",
    "There are two fixes:\n",
    "* if `y_score` is an `np.array`, we can pass in `-y_score` to put the ordering the way *sklearn* expects it.\n",
    "* we can set the `pos_label` to 0 (easy to do with a keyword argument, but harder for next reader of our code to understand)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T22:23:07.938000Z",
     "start_time": "2018-12-21T22:23:07.918349Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "print(f\"\"\"\n",
    "Setting the pos_label to class 0: {average_precision_score(y_true, y_scores, pos_label=0)}\n",
    "Using the negative score:         {average_precision_score(y_true, -y_scores)}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision@n for the shuttle data set\n",
    "\n",
    "Let's look at what P@n and Average Precision look like on the shuttle dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T22:23:11.107408Z",
     "start_time": "2018-12-21T22:23:07.940448Z"
    }
   },
   "outputs": [],
   "source": [
    "# reset the scores to those produced by our model\n",
    "y_scores = iso_forest.decision_function(df.drop('anomaly', axis=1))\n",
    "\n",
    "num_anomalies = sum(df['anomaly'])\n",
    "average_precision = average_precision_score(df['anomaly'], -y_scores)\n",
    "plt.figure(dpi=150)\n",
    "\n",
    "plt.plot(precision_at_n(df['anomaly'], y_scores), label='P@n')\n",
    "plt.axhline(average_precision, linestyle='--', color='k', \n",
    "            label='average precision')\n",
    "plt.axvline(num_anomalies, linestyle='-.', color='k', label='number of anomalies')\n",
    "plt.xlim(0, num_anomalies + 1500)\n",
    "plt.xlabel('n: number of items')\n",
    "plt.ylabel('P@n')\n",
    "plt.legend()\n",
    "plt.title('P@n vs n; Average precision and total # of anomalies indicated');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T22:23:11.113627Z",
     "start_time": "2018-12-21T22:23:11.109583Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"The value for average precision is {average_precision:6.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that P@n depends on the number of items $n$ quite strongly for small n ($n \\ll 50$), but then stays relatively constant until $n\\sim 869$, the number of anomalies in the data. After that, putting more items from the ranked list just decreases the average precision as the power law $1/n$.\n",
    "\n",
    "The average precision is only averaged at the location of the anomalies (i.e. early in the dataset), so it is a lot higher than the average of P@n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about focusing on that dip at the beginning when $n$ is small?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T22:23:14.212474Z",
     "start_time": "2018-12-21T22:23:11.118336Z"
    }
   },
   "outputs": [],
   "source": [
    "# reset the scores to those produced by our model\n",
    "y_scores = iso_forest.decision_function(df.drop('anomaly', axis=1))\n",
    "\n",
    "num_anomalies = sum(df['anomaly'])\n",
    "average_precision = average_precision_score(df['anomaly'], -y_scores)\n",
    "plt.figure(dpi=150)\n",
    "\n",
    "plt.plot(precision_at_n(df['anomaly'], y_scores), label='P@n', marker='o')\n",
    "plt.axhline(average_precision, linestyle='--', color='k', \n",
    "            label='average precision')\n",
    "plt.xlim(0, 15)\n",
    "plt.xlabel('n: number of items')\n",
    "plt.ylabel('P@n')\n",
    "plt.legend()\n",
    "plt.title('P@n vs n; Average precision indicated');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At low values of $n$, P@n can be very noisy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T01:31:24.079431Z",
     "start_time": "2018-12-10T01:31:24.074263Z"
    }
   },
   "source": [
    "### The adjusted P@n and adjusted average precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T01:31:26.666888Z",
     "start_time": "2018-12-10T01:31:26.639545Z"
    }
   },
   "source": [
    "This is not provided by *sklearn* either, but the manipulations to get adjusted P@n from raw P@n are simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T22:23:17.178881Z",
     "start_time": "2018-12-21T22:23:14.218915Z"
    }
   },
   "outputs": [],
   "source": [
    "# reset the scores to those produced by our model\n",
    "y_scores = iso_forest.decision_function(df.drop('anomaly', axis=1))\n",
    "\n",
    "num_anomalies = sum(df['anomaly'])\n",
    "num_points = df['anomaly'].size\n",
    "frac_anomalies = num_anomalies / num_points\n",
    "\n",
    "p_at_n = np.array(precision_at_n(df['anomaly'], y_scores))\n",
    "adj_p_at_n = (p_at_n - frac_anomalies) / (1 - frac_anomalies)\n",
    "\n",
    "ave_precision = average_precision_score(df['anomaly'], -y_scores)\n",
    "adj_ave_precision = (ave_precision - frac_anomalies) / (1 - frac_anomalies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T22:23:17.475452Z",
     "start_time": "2018-12-21T22:23:17.184961Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(dpi=150)\n",
    "\n",
    "plt.plot(p_at_n, label='P@n')\n",
    "plt.plot(adj_p_at_n, label='Adjusted P@n')\n",
    "plt.xlabel('n: number of items')\n",
    "plt.ylabel('P@n')\n",
    "plt.xlim(0, 100)\n",
    "plt.legend()\n",
    "plt.title('P@n vs n for a very good classifier');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This classifier does such a good job, we are getting close to optimal performance. We can train a worse version of isolation forest for comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T22:23:19.709945Z",
     "start_time": "2018-12-21T22:23:17.479335Z"
    }
   },
   "outputs": [],
   "source": [
    "bad_iso_forest = IsolationForest(n_estimators = 3, behaviour='new', contamination=frac_anomalies, random_state=42)\n",
    "bad_iso_forest.fit(df.drop('anomaly', axis=1))\n",
    "bad_y_scores = bad_iso_forest.decision_function(df.drop('anomaly', axis=1))\n",
    "\n",
    "p_at_n = np.array(precision_at_n(df['anomaly'], bad_y_scores))\n",
    "adj_p_at_n = (p_at_n - frac_anomalies) / (1 - frac_anomalies)\n",
    "\n",
    "ave_precision = average_precision_score(df['anomaly'], -bad_y_scores)\n",
    "adj_ave_precision = (ave_precision - frac_anomalies) / (1 - frac_anomalies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T22:23:19.956759Z",
     "start_time": "2018-12-21T22:23:19.711861Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(dpi=150)\n",
    "\n",
    "plt.plot(p_at_n, label='P@n')\n",
    "plt.plot(adj_p_at_n, label='Adjusted P@n')\n",
    "plt.xlabel('n: number of items')\n",
    "plt.ylabel('P@n')\n",
    "plt.xlim(0, 15)\n",
    "plt.legend()\n",
    "plt.title('P@n vs n for a bad classifier');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As n is small, we see the effect of applying the “adjustment” (i.e. how well the classifier performs compared to random rankings).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Same data, different algorithm\n",
    "\n",
    "Let's use some of the distance based algorithms we have seen earlier (lesson 4). To make _good_ anomaly detection methods, we should apply \n",
    "- scaling of features\n",
    "- maybe apply PCA for dimensional reduction.\n",
    "\n",
    "We will take a naive approach and score the results using P@n and average precision. Then we will treat the data again with the proper preprocessing steps, and see how the results improve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T22:23:19.965381Z",
     "start_time": "2018-12-21T22:23:19.959917Z"
    }
   },
   "outputs": [],
   "source": [
    "# from lecture 4, where we use the average distance of a point to its n_neighbors to score it\n",
    "def do_nn_avg_outlier_scores(obs, n_neighbors=1):\n",
    "    nn = neighbors.NearestNeighbors(n_neighbors=n_neighbors)\n",
    "    nn.fit(obs)\n",
    "    dists, idx = nn.kneighbors()\n",
    "    scores = dists.mean(axis=1)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T22:23:19.971268Z",
     "start_time": "2018-12-21T22:23:19.967603Z"
    }
   },
   "outputs": [],
   "source": [
    "features = df.drop('anomaly', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T22:23:22.939100Z",
     "start_time": "2018-12-21T22:23:19.973416Z"
    }
   },
   "outputs": [],
   "source": [
    "# we want lower numbers (i.e. larger negative distances) to be more likely to be outliers\n",
    "y_scores = -do_nn_avg_outlier_scores(features, n_neighbors=20)\n",
    "\n",
    "p_at_n = np.array(precision_at_n(df['anomaly'], y_scores))\n",
    "adj_p_at_n = (p_at_n - frac_anomalies) / (1 - frac_anomalies)\n",
    "\n",
    "ave_precision = average_precision_score(df['anomaly'], -y_scores)\n",
    "adj_ave_precision = (ave_precision - frac_anomalies) / (1 - frac_anomalies)\n",
    "\n",
    "plt.figure(dpi=150)\n",
    "\n",
    "plt.plot(p_at_n, label='P@n')\n",
    "plt.plot(adj_p_at_n, label='Adjusted P@n')\n",
    "\n",
    "plt.axhline(adj_ave_precision, linestyle='--', color='k', \n",
    "            label='adjusted average precision')\n",
    "plt.xlim(0, 1000)\n",
    "plt.xlabel('n: number of items')\n",
    "plt.ylabel('P@n')\n",
    "plt.legend()\n",
    "plt.title('P@n vs n for kNN average distance');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T22:25:20.805251Z",
     "start_time": "2018-12-21T22:23:22.941567Z"
    }
   },
   "outputs": [],
   "source": [
    "# scan for the best value of n (may take a while)\n",
    "ap = [(average_precision_score(df['anomaly'], do_nn_avg_outlier_scores(features, n_neighbors=n)), n)\n",
    "      for n in range(2, 100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T22:25:20.830264Z",
     "start_time": "2018-12-21T22:25:20.811028Z"
    }
   },
   "outputs": [],
   "source": [
    "sorted(ap, reverse=True)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we get the best results for average precision with 99 neighbors!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using kNN again, but with scaling and dimensional reduction\n",
    "\n",
    "As discussed in Lesson 5, we need to do standard scaling when using distance based methods. The kNN method above demonstrates a _bad_ anomaly detector. We can see how our metrics can measure the difference between kNN done properly (with scaling and dimensional reduction) versus a naive method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T22:25:20.889506Z",
     "start_time": "2018-12-21T22:25:20.832946Z"
    }
   },
   "outputs": [],
   "source": [
    "ssX = StandardScaler()\n",
    "features_scaled = ssX.fit_transform(features.astype(float))\n",
    "\n",
    "# How many features do we need?\n",
    "pca = PCA()\n",
    "pca.fit(features_scaled)\n",
    "np.cumsum(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like 6 components captures 99.9% of the variance, so we will reduce the dimensionality of our data accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T22:25:20.952989Z",
     "start_time": "2018-12-21T22:25:20.891612Z"
    }
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=6)\n",
    "pca_features = pca.fit_transform(features_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us try some different values of $k$ to see how many neighbors we should use. We will keep track of the average precision score, and the value of $k$ that gave us that score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T22:26:49.848646Z",
     "start_time": "2018-12-21T22:25:20.956465Z"
    }
   },
   "outputs": [],
   "source": [
    "# Warning: this cell takes about 90 seconds to run!\n",
    "ap = [(average_precision_score(df['anomaly'], do_nn_avg_outlier_scores(pca_features, n_neighbors=n)), n) \n",
    "      for n in range(100, 200)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T22:26:49.864602Z",
     "start_time": "2018-12-21T22:26:49.850497Z"
    }
   },
   "outputs": [],
   "source": [
    "# Now look at the average precision score for different numbers of nearest neighbors\n",
    "sorted(ap, reverse=True)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T22:38:35.664868Z",
     "start_time": "2018-12-21T22:38:35.403285Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(dpi=150)\n",
    "plt.plot(np.array(ap)[:,1], np.array(ap)[:, 0])\n",
    "plt.title('Looking for best # neighbors for kNN')\n",
    "plt.ylabel('Average precision')\n",
    "plt.xlabel('Number of neighbors for kNN');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score keeps increasing with the number of neighbors (i.e. we should keep increasing the number of neighbors beyond 200). Some experimentation shows that `n_neighbors` $\\approx 600$ maximizes the average precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T22:40:28.601612Z",
     "start_time": "2018-12-21T22:40:23.784247Z"
    }
   },
   "outputs": [],
   "source": [
    "y_scores = -do_nn_avg_outlier_scores(pca_features, n_neighbors=600)\n",
    "\n",
    "p_at_n = np.array(precision_at_n(df['anomaly'], y_scores))\n",
    "adj_p_at_n = (p_at_n - frac_anomalies) / (1 - frac_anomalies)\n",
    "\n",
    "ave_precision = average_precision_score(df['anomaly'], -y_scores)\n",
    "adj_ave_precision = (ave_precision - frac_anomalies) / (1 - frac_anomalies)\n",
    "\n",
    "plt.figure(dpi=150)\n",
    "\n",
    "plt.plot(p_at_n, label='P@n')\n",
    "plt.plot(adj_p_at_n, label='Adjusted P@n')\n",
    "\n",
    "plt.axhline(adj_ave_precision, linestyle='--', color='k', \n",
    "            label='adjusted average precision')\n",
    "plt.xlim(0, 1000)\n",
    "plt.xlabel('n: number of items')\n",
    "plt.ylabel('P@n')\n",
    "plt.legend()\n",
    "plt.title('P@n vs n for kNN average distance');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using ROC\n",
    "\n",
    "One of the standard metrics commonly used for both classification and anomaly detection is the Receiver Operator Characteristic Curve (ROC Curve). The curve shows the tradeoff between the false positive and false negative rates as the threshold for the score is varied.\n",
    "\n",
    "The threshold can either be chosen as a cutoff score (typical for classification) or a cutoff rank (typical for anomaly detection problems).\n",
    "\n",
    "The _area under the curve_ gives us a single number that tells us how well the classifier does at distinguishing between the two cases. The number can be interpreted as the probability that a randomly chosen anomaly has a higher score than a randomly chosen normal point.\n",
    "\n",
    "\\begin{equation*}\n",
    "\\text{AUC} = P(\\text{score of random anomaly} > \\text{score of random inlier})\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T22:40:28.916011Z",
     "start_time": "2018-12-21T22:40:28.908613Z"
    }
   },
   "outputs": [],
   "source": [
    "roc_model = namedtuple('roc_model', 'name model_scores')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the ROC AUC scores for the different models is straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T22:40:32.823547Z",
     "start_time": "2018-12-21T22:40:29.160666Z"
    }
   },
   "outputs": [],
   "source": [
    "roc_models = [\n",
    "    roc_model('Isolation Forest', -iso_forest.decision_function(features)),\n",
    "    roc_model('kNN no proprocessing', do_nn_avg_outlier_scores(features, n_neighbors=20)),\n",
    "    roc_model('kNN processing', do_nn_avg_outlier_scores(pca_features, n_neighbors=600))\n",
    "]\n",
    "\n",
    "for model in roc_models:\n",
    "    label = f\"{model.name} ({roc_auc_score(df['anomaly'], model.model_scores):6.4f})\"\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also make the plot with all three ROC curves on it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T22:40:33.545895Z",
     "start_time": "2018-12-21T22:40:33.133078Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(dpi=180)\n",
    "\n",
    "plt.plot([0,1], [0,1], 'k--', label='random guess (0.5000)')\n",
    "\n",
    "for model in roc_models:\n",
    "    label = f\"{model.name} ({roc_auc_score(df['anomaly'], model.model_scores):6.4f})\"\n",
    "    fpr, tpr, _ = roc_curve(df['anomaly'], model.model_scores)\n",
    "    plt.plot(fpr, tpr, label=label)\n",
    "\n",
    "plt.xlabel(\"False positive rate\")\n",
    "plt.ylabel(\"True positive rate\")\n",
    "plt.title(\"ROC curves for different anomaly detectors\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final comments \n",
    "\n",
    "* Python has a built-in way of generating ROC curves. You will need to write your own functions for precision@n and average precision\n",
    "* Many datasets that you want to perform anomaly detection on don't have the anomalies labeled. You can process your metrics on classification tasks with similar distributions of data, although you may need to downsample the majority class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise #1\n",
    "\n",
    "We don't cross-validate the results when fitting hyperparmaters for anomaly detection. Instead, we set a \"reasonable range\" of hyperparameters (usually determined by experience or a subject matter expert), then look at our metrics as the average over all parameter choice. \n",
    "\n",
    "Use $k=500$ to $k=600$ for the processed kNN model.\n",
    "* What is the mean of the average precision (i.e. take the mean over the 100 models with k ranging from 500 to 600)\n",
    "* What is the standard error of the average precision (i.e. find the standard error over the same 100 models).\n",
    "\n",
    "Reminder: standard error is\n",
    "$$\\text{std err} = \\frac{\\sigma}{\\sqrt{N}}$$\n",
    "where $\\sigma$ is the standard deviation and $N$ is the number of models you are averaging over"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise #2\n",
    "\n",
    "Make a Local Outlier Factor anomaly detector on the (scaled) shuttle dataset, `pca_features`. What is the area under the ROC curve? Take the number of neighbors to range from 2000 to 3500 in steps of 100, and give the mean and standard error. \n",
    "\n",
    "**Resources:** We discussed Local Outlier Factor models in lesson 4, or you can find the documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise #3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T23:31:24.382583Z",
     "start_time": "2018-12-21T23:31:24.377172Z"
    }
   },
   "source": [
    "We have given the interpretation of an ROC score as \n",
    "> The probability that a randomly selected \"normal\" point has a higher score than a randomly selected \"anomaly\".\n",
    "\n",
    "Let's check this interpretation. Below is a set of scores for normal points, and a set of scores for anomalies. Calculate the ROC curves using two methods:\n",
    "1. Using sklearn's `roc_auc_score` method. You will need to join the scores somehow.\n",
    "2. Randomly drawing a normal point and an anomaly, and comparing the scores. Repeat this 10000 times (with replacement). Give the fraction of times the anomaly had the higher score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-22T00:57:29.365781Z",
     "start_time": "2018-12-22T00:57:29.359184Z"
    }
   },
   "outputs": [],
   "source": [
    "normal_scores = np.random.normal(loc=700, scale=50,size=2000)\n",
    "anomaly_scores = np.random.normal(loc=800, scale=20,size=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution #3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "In this assignment you should have learned: \n",
    "\n",
    "1. How to apply the metrics Precision@n, average precision, and ROC scores\n",
    "2. To generate anomaly detection data from classification datasets for baseline evaluation\n",
    "3. To do parameter scanning without doing cross-validation\n",
    "\n",
    "Congratulations! This concludes the lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Intel Anomaly Env",
   "language": "python",
   "name": "intel_anomaly"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
