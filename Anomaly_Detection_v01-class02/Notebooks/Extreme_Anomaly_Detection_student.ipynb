{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2019 Intel Corporation\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In Lesson 2 of *Anomaly Detection*, we learned that extreme value analysis can be used to estimate the chance of a rare event happening. We also discussed angle-based and depth-based anomaly detection. While the three approaches are different, they share one feature: they assume that the anomalies lie at the border (or edge or tail) of the data.\n",
    "\n",
    "Here will apply extreme value analysis to better understand rare events in atmospheric ozone levels (univariate data). We will also use angle-based and depth-based anomaly detection to find anomalies in multivariate data.\n",
    "\n",
    "# Learning Outcomes\n",
    "\n",
    "You should walk away from this Python tutorial with:\n",
    "1. An understanding of extreme value analysis\n",
    "2. Some practical experience with angle-based anomaly detection \n",
    "3. Some practical experience with depth-based anomaly detection \n",
    "\n",
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-10T18:56:26.059905Z",
     "start_time": "2019-01-10T18:56:24.809768Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import datetime\n",
    "import scipy\n",
    "import scipy.stats as ss\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python & Library Versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-10T18:56:26.088983Z",
     "start_time": "2019-01-10T18:56:26.077746Z"
    }
   },
   "outputs": [],
   "source": [
    "packages = [matplotlib, np, pd, scipy]\n",
    "\n",
    "msg = f\"\"\"\n",
    "Python Version: {sys.version}\n",
    "\n",
    "library        version\n",
    "-------        -------\"\"\"\n",
    "print(msg)\n",
    "\n",
    "for package in packages:\n",
    "    print(f\"{package.__name__:11}    {package.__version__:>7}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Extreme Value Analysis\n",
    "\n",
    "In this section, we use extreme value analysis to detect anomalies in real-world data: ozone levels in the New York metropolitan area. Near ground level, ozone is a respiratory hazard as it can cause damage to mucous and respiratory tissues. The air quality index (AQI) for ozone is a scale from 0 to 500 that describes the level of ozone pollution: the higher the number, the greater the hazard. More details about the scale can be found here:\n",
    "\n",
    "https://www.airnow.gov/aqi/aqi-basics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ozone AQI levels for the New York metropolitan area are taken from the US Environmental Protection Agency website:\n",
    "\n",
    "https://www.epa.gov/outdoor-air-quality-data/air-quality-index-daily-values-report\n",
    "\n",
    "(Geographic Area is \"New York-Newark-Jersey City\")\n",
    "\n",
    "All full years available were selected (1980-2017) and the data for all years were combined into a single CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data as a pandas dataframe and take a look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-10T18:56:29.121990Z",
     "start_time": "2019-01-10T18:56:27.535390Z"
    }
   },
   "outputs": [],
   "source": [
    "ozone_aqi = pd.read_csv('aqidailyozone.csv', \n",
    "                        index_col=0, \n",
    "                        parse_dates=True) \n",
    "ozone_aqi.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-10T18:56:29.157465Z",
     "start_time": "2019-01-10T18:56:29.147504Z"
    }
   },
   "outputs": [],
   "source": [
    "ozone_aqi.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's perform two quick checks on the data. First, look for missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-10T18:56:29.191308Z",
     "start_time": "2019-01-10T18:56:29.182072Z"
    }
   },
   "outputs": [],
   "source": [
    "ozone_aqi.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good. No missing values. Second, check that the number of rows is equal to number of days from 1/1/80 to 12/31/17 (inclusive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-10T18:56:29.251857Z",
     "start_time": "2019-01-10T18:56:29.243814Z"
    }
   },
   "outputs": [],
   "source": [
    "d0 = datetime.date(1980, 1, 1)\n",
    "d1 = datetime.date(2017, 12, 31)\n",
    "difference = d1 - d0\n",
    "# Last day is not included in the difference, so add one to account for it\n",
    "print('Number of days is {}'.format(difference.days+1))\n",
    "print('Number of rows is {}'.format(ozone_aqi.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** in principle we also must check that the data is stationary, but such checks are beyond the scope of this lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now we can proceed to analyze the data. As we discussed in the lecture, there are two main approaches to extreme value analysis: \n",
    "* a) block maxima  \n",
    "* b) peak over threshold. \n",
    "\n",
    "We will examine our data using both approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1a: Block Maxima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start with block maxima. The first step is to generate the annual maxima series (AMS), which consists of the maximum ozone AQI value for each year. We extract the AMS from the daily data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T18:35:45.123573Z",
     "start_time": "2018-12-05T18:35:45.103053Z"
    }
   },
   "outputs": [],
   "source": [
    "year_grouper = pd.Grouper(freq='A')\n",
    "ams = ozone_aqi.groupby(year_grouper).max()\n",
    "print(ams)\n",
    "print('')\n",
    "print('Number of AMS entries is {}'.format(len(ams)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AMS has 38 entries as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** Grouping by maximum has also produced the maximum in the index (i.e. the date is always December 31st, as that is the maximum date). These two maxima are taken independently, we are _not_ just getting the AQI on December 31st."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the AMS to a generalized extreme value (GEV) distribution. We'll use the statistical functions in scipy.stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T18:35:45.196930Z",
     "start_time": "2018-12-05T18:35:45.127199Z"
    }
   },
   "outputs": [],
   "source": [
    "fit = ss.genextreme.fit(ams)\n",
    "print(fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the shape, location and scale parameters, respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note:* the shape parameter used in scipy has the opposite sign to the one used in most other reference sources and software packages. See\n",
    "\n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.genextreme.html#scipy.stats.genextreme\n",
    "\n",
    "and\n",
    "\n",
    "https://github.com/scipy/scipy/issues/3844\n",
    "\n",
    "In other words, the scipy shape parameter, which is denoted by c = -$\\xi$, the shape parameter we used in the Lesson 2 PowerPoint presentation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a histogram of the data and the fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T18:35:45.396396Z",
     "start_time": "2018-12-05T18:35:45.201138Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(ams.iloc[:,0], bins=5, density=True, alpha=0.7, label='Data')\n",
    "plt.plot(np.linspace(100, 300, 100),\n",
    "         ss.genextreme.pdf(np.linspace(100, 300, 100), fit[0], fit[1], fit[2]), 'r--',\n",
    "         label='GEV fit')\n",
    "plt.title('GEV Distribution', fontweight='bold')\n",
    "plt.xlabel('Ozone AQI value')\n",
    "plt.ylabel('Probability')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fit captures the general trend, but it is not great as can be seen by eye. (Further statistical analysis of the quality of the fit is beyond the scope of this lesson.)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: What is one reason for the problems with the fit?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: One reason is the small dataset (38 points). We also have to be careful how we bin the data for the histogram. Try different values of *bins* and see what happens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the problems, we can still ask questions that are relevant to human health. For example, what is the probability of exceeding an AQI of 200 in a given year? (Ozone levels above 200 are considered very unhealthy.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To answer this question, calculate the cumulative distribution function (CDF). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T18:35:45.403775Z",
     "start_time": "2018-12-05T18:35:45.398913Z"
    }
   },
   "outputs": [],
   "source": [
    "prob_over_200 = (1.0 - ss.genextreme.cdf(200, fit[0], fit[1], fit[2]))\n",
    "print('{:.2f}'.format(prob_over_200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might also be concerned about how bad are rare events. For example, what ozone AQI has one percent probability of being exceeded in a given year? This is the same as asking for the AQI at which the CDF is 0.99. We can use the _inverse_ of the CDF, called the _percent point function_ (PPF) to answer this question.\n",
    "\n",
    "In more detail:\n",
    "> \"Question: What value of X has CDF(X) = 0.99?\"\n",
    ">\n",
    "> \"Answer: PPF(0.99)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T18:35:45.411186Z",
     "start_time": "2018-12-05T18:35:45.406131Z"
    }
   },
   "outputs": [],
   "source": [
    "boundary = ss.genextreme.ppf(0.99, fit[0], fit[1], fit[2])\n",
    "print(f'The one precent threshold ozone AQI level is {boundary:6.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Section 1b: Peak Over Threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if you are not interested in the maxima in specific blocks, but want to know about exceeding a threshold more generally? For example, in the ozone case, an exceedance might occur several times a year or not at all in a given year. For such scenarios, the peak of threshold (POT) approach is more appropriate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at all of the ozone data as histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T18:35:45.599005Z",
     "start_time": "2018-12-05T18:35:45.413588Z"
    }
   },
   "outputs": [],
   "source": [
    "ozone_aqi['Ozone AQI Value'].hist();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "source": [
    "We note that most of the data is below an AQI of 100, which is considered moderate to good air quality. (Phew!) We are concerned about the high AQI tail, which can be a health hazard. As we discussed in the lecture, this tail should be well approximated by a universal distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task of choosing the threshold for the tail requires some judgement. If it is too low, then the theorems about the universal behavior of the tail do not apply. If it is too high, then there will be few data point in the tail and the resulting estimates for the parameters will be poor. Often, it is a matter of trying different thresholds and looking for a good (enough) fit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will use a convenient rule of thumb for where to start the exploration: take the lowest value of the AMS. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T18:35:45.606017Z",
     "start_time": "2018-12-05T18:35:45.601797Z"
    }
   },
   "outputs": [],
   "source": [
    "min_max = ams.values.min()\n",
    "print(f'Lowest maxima value for Ozone AQI in any year: {min_max}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are going to try different values of the threshold, define an appropriate function to create the tail and plot it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T18:35:45.612943Z",
     "start_time": "2018-12-05T18:35:45.608914Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_tail(threshold):\n",
    "    ozone_aqi_over_threshold = ozone_aqi[ozone_aqi['Ozone AQI Value'] > threshold]\n",
    "    ozone_aqi_over_threshold.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T18:35:45.825879Z",
     "start_time": "2018-12-05T18:35:45.614699Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_tail(min_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expect the tail to be a decreasing function of the ozone AQI level and here we see a plateau from 170 to 210. Therefore, we will increase the threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T18:35:46.041458Z",
     "start_time": "2018-12-05T18:35:45.830614Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_tail(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks more reasonable. As we discussed in lectures, let us first try to fit an exponential distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T18:35:46.093295Z",
     "start_time": "2018-12-05T18:35:46.046218Z"
    }
   },
   "outputs": [],
   "source": [
    "threshold = 200\n",
    "ozone_aqi_threshold = ozone_aqi[ozone_aqi['Ozone AQI Value'] > threshold].iloc[:,0]\n",
    "fit_expon = ss.expon.fit(ozone_aqi_threshold)\n",
    "print(fit_expon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the location $\\mu$ and scale $\\sigma$ parameters, respectively, for an exponential distribution of the form $exp [-(x-\\mu)/\\sigma]$.\n",
    "\n",
    "Plot the results together with the exponential fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T18:35:46.300967Z",
     "start_time": "2018-12-05T18:35:46.096547Z"
    }
   },
   "outputs": [],
   "source": [
    "# linspace starts at 201 (one above threshold) to avoid fit going to zero, which looks ugly\n",
    "\n",
    "plt.hist(ozone_aqi_threshold, bins=10, density=True, alpha=0.7, label='Tail')\n",
    "plt.plot(np.linspace(201, 280, 100),\n",
    "         ss.expon.pdf(np.linspace(201, 280, 100), fit_expon[0], fit_expon[1]), 'r--',\n",
    "         label='Exponential fit')\n",
    "plt.title('Exponential Distribution', fontweight='bold')\n",
    "plt.xlabel('Ozone AQI value')\n",
    "plt.ylabel('Probability')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad. You should repeat the analysis above with different thresholds. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: How would you decide which threshold to use from all of those you explored?\n",
    "\n",
    "**Answer:** One way to choose a threshold is the one which gives the smallest mean squared error between the data and the fit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the block maxima approach, we can now ask questions about different outcomes. Let's revisit the questions we asked before and compare answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "A. What is the probability of exceeding an AQI of 200 in the time period examined (38 years)? \n",
    "Here the answer is 1.00, since we choose our threshold to be 200.\n",
    "\n",
    "Previously, we found that the probability of exceeding 200 in a given year was 0.75. Therefore, the probability of exceeding 200 at least once in 38 years is $1 - (1 - 0.75)^{38} \\approx 1.00$, which is the same as above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B. Using the block maxima approach, we found that the ozone AQI which has a one percent probability of being exceeded in a given year is 281. Therefore, the probability that this level won't be exceeded over 38 years is $(1 - 0.01)^{38} \\approx 0.68$.\n",
    "\n",
    "From the POT analysis, the probability that the AQI level of 281 won't be exceeded over 38 years is given by\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T18:35:46.314965Z",
     "start_time": "2018-12-05T18:35:46.305595Z"
    }
   },
   "outputs": [],
   "source": [
    "ss.expon.cdf(281, fit_expon[0], fit_expon[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is significantly higher that the block maxima estimate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Why is there a difference?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: One reason has to do with the shape of the GEV vs. that of the POT distribution. The former goes to zero at finite value (289 in our case), while the POT does not. As a result, small changes in the AQI threshold near 289 have a large effect of the GEV probability, but don't have as big an effect on the POT-derived probability.\n",
    "\n",
    "For example, the 0.1% threshold from GEV is 287. The probability that this level won't be exceeded over 38 years is  $(1-0.001)^{38} \\approx 0.96$.\n",
    "\n",
    "For POT, the same probability is 0.996, which is close to the GEV value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For completeness, we can fit the data with a generalized Pareto distribution (GPD). As we discussed in lectures, the exponential distribution is a special case of the GPD (shape parameter is zero)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T18:35:46.376929Z",
     "start_time": "2018-12-05T18:35:46.319223Z"
    }
   },
   "outputs": [],
   "source": [
    "fit2 = ss.genpareto.fit(ozone_aqi_threshold, 20, loc=201, scale=16)\n",
    "print(fit2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the shape $\\xi$, location $\\mu$ and scale $\\sigma$ parameters, respectively, of the GPD. Note that $\\xi$ is very close to zero, which suggests that our initial approach of using an exponential distribution was reasonable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T18:35:46.626812Z",
     "start_time": "2018-12-05T18:35:46.379752Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(ozone_aqi_threshold, bins=10, density=True, alpha=0.7, label='Tail')\n",
    "plt.plot(np.linspace(201, 280, 100),\n",
    "         ss.genpareto.pdf(np.linspace(201, 280, 100), fit2[0], fit2[1], fit2[2]), 'r--',\n",
    "         label='GPD fit')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very similar to what we found before for the exponential distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: for a GPD fit to work, we need to specify initial guesses for all of the parameters.  For the location and scale parameters, we chose the values we found for the exponential distribution. Try using different initial guesses and see what you get. Make sure you plot the fit and compare it with the data to check that *genpareto.fit* is producing reasonable output (sometimes it doesn't)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Section 2: Angle-Based Anomaly Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we use angle-based anomaly detection to detect anomalies in simulated two-dimensional data. A reminder how angle-based anomaly detection works:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Choose a point in the data\n",
    "2. Calculate all angles that this point makes with other pairs of points in the data\n",
    "3. Calculate the variance of these angles\n",
    "4. Anomalies have low variance; normal points have high variance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, you will have training data where you know the anomalies and so can derive a threshold for the variance below which the points will be classified as anomalies. You then apply this threshold to your test data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since here our focus is the algorithm and not the data, we won't carry out a test/train split. Instead, we will use simulated data where we know the anomalies and see how well the algorithm performs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by creating the data, which will be a combination of normal points and anomalies. Each set of points (normal and anomaly) is generated from a 2D Gaussian distribution where we specify the mean in each dimension as well as the covariance matrix. We choose the normal data to be 100 points tightly clumped together, while the anomalies are 10 points that are further away and spread out more broadly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T18:35:46.641987Z",
     "start_time": "2018-12-05T18:35:46.633560Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(16) # include a seed for reproducibility\n",
    "\n",
    "# generate the normal data\n",
    "normal_mean = np.array([1.0, 2.0]) \n",
    "normal_covariance = np.array([[0.2, 0.0], [0.0, 0.1]])\n",
    "normal_data = np.random.multivariate_normal(normal_mean, normal_covariance, 100)\n",
    "\n",
    "# generate the anomalous data\n",
    "anomaly_mean = np.array([6.0, 8.0]) \n",
    "anomaly_covariance = np.array([[2.0, 0.0], [0.0, 4.0]])\n",
    "anomaly_data = np.random.multivariate_normal(anomaly_mean, anomaly_covariance, 10)\n",
    "\n",
    "# Note:\n",
    "#\n",
    "# Do not confuse the two uses of 'normal'\n",
    "# The 'normal' in np.random.multivariate_normal refers to a Gaussian distribution\n",
    "# and is not related to normal vs. anomaly\n",
    "#\n",
    "\n",
    "# Combine the data into one array for later use\n",
    "all_data = np.concatenate((normal_data, anomaly_data), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the data and color code normal vs. anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T18:35:46.768917Z",
     "start_time": "2018-12-05T18:35:46.645151Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,4))\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "ax1.scatter(normal_data[:,0], normal_data[:,1], s=30, c='b', marker=\"o\", label='normal')\n",
    "ax1.scatter(anomaly_data[:,0], anomaly_data[:,1], s=30, c='r', marker=\"o\", label='anomaly')\n",
    "plt.legend(loc='upper left');\n",
    "plt.xlabel('x', fontsize=12)\n",
    "plt.ylabel('y', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first implement a true angle-based algorithm. That is, we calculate the actual angles for each set of three points in the data. As you will see, this algorithm performs poorly, which leads us to consider an alternative: the distance-weighted angle-based anomaly detection that we discussed in lectures. In Exercise 2, you will be asked implement this distance-weighted algorithm and check how well it identifies anomalies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by constructing the two key functions for the angle-based anomaly detection algorithm. The first function, *angle*, calculates the angle between three points. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T18:35:46.776385Z",
     "start_time": "2018-12-05T18:35:46.771390Z"
    }
   },
   "outputs": [],
   "source": [
    "# Given points A, B and C, this function returns the acute angle between vectors AB and AC\n",
    "# using the dot product between these vectors\n",
    "\n",
    "def angle(point1, point2, point3): \n",
    "    v21 = np.subtract(point2, point1)\n",
    "    v31 = np.subtract(point3, point1)\n",
    "    dot_product = (v21*v31).sum()\n",
    "    normalization = np.linalg.norm(v21)*np.linalg.norm(v31)\n",
    "    acute_angle = np.arccos(dot_product/normalization)\n",
    "    return acute_angle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second function, *eval_angle_point*, takes two inputs: a point and data (a collection of points). This function returns a list of angles that the input point makes with all pairs of points in the input data. It uses *angle* of the angle calculation.\n",
    "\n",
    "A simple loop through all pairs of points in the data is insufficient because we must make sure the three points lead to well-determined angles without repetition. To be more specific, if we choose point A and then are considering two points in the data B (second point) and C (third point), we want to make sure that the following conditions hold:\n",
    "\n",
    "1. Point B is different from A\n",
    "2. Point C is different from A and B\n",
    "3. We don't calculate the same angle twice. That is, if we calculate the angle between vectors AB and AC (where B is the second point and C is the third point), we don't repeat the calculation with vectors AC and AB (where C is the second point and B is the third point).\n",
    "\n",
    "The *if* statments in *eval_angle_point* ensure the three conditions hold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T18:35:46.783647Z",
     "start_time": "2018-12-05T18:35:46.779106Z"
    }
   },
   "outputs": [],
   "source": [
    "def eval_angle_point(point, data):\n",
    "    angles_data = []\n",
    "    for index_b, b in enumerate(data):\n",
    "        if (np.array_equal(b, point)):\n",
    "            continue\n",
    "        # ensure point c comes later in array that point b\n",
    "        # so we don't double count points\n",
    "        for c in data[index_b + 1:]:\n",
    "            if (np.array_equal(c, point)) or (np.array_equal(c, b)):\n",
    "                continue\n",
    "            angles_data.append(angle(point, b, c))\n",
    "    return angles_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use *eval_angle_point* with one point from the normal data and another from the anomaly data to illustrate the claim we made above: anomalies have lower angle variance than normal points. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose a normal point and an anomaly at random and plot the angles made for 100 pairs of points in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-10T18:56:36.619946Z",
     "start_time": "2019-01-10T18:56:35.946977Z"
    }
   },
   "outputs": [],
   "source": [
    "fig2 = plt.figure(figsize=(6,4))\n",
    "ax2 = fig2.add_subplot(111)\n",
    "np.random.seed(17)\n",
    "normal_point = random.choice(normal_data)\n",
    "anomaly_point = random.choice(anomaly_data)\n",
    "normal_angles = eval_angle_point(normal_point, all_data)\n",
    "anomaly_angles = eval_angle_point(anomaly_point, all_data)\n",
    "ax2.plot(normal_angles[0:100],  marker=\"o\", label='normal')\n",
    "ax2.plot(anomaly_angles[0:100], marker=\"o\", label='anomaly')\n",
    "plt.xlabel('label', fontsize=12)\n",
    "plt.ylabel('angle (rad)', fontsize=12)\n",
    "plt.legend(loc='best')\n",
    "plt.savefig('angle_based.png', dpi=600) # for use in the lecture\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T18:35:47.779863Z",
     "start_time": "2018-12-05T18:35:47.772887Z"
    }
   },
   "outputs": [],
   "source": [
    "print('The normal point is {}'.format(normal_point))\n",
    "print('The variance in angle for the normal point is {:.5f}'.format(np.var(normal_angles)))\n",
    "print('The anomaly point is {}'.format(anomaly_point))\n",
    "print('The variance in angle for the anomaly point is {:.5f}'.format(np.var(anomaly_angles)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This two point comparison looks promising, but is not statistically meaningful. \n",
    "\n",
    "Let's calculate the angle variance for all the anomaly data. We will label the anomalies with type '0' so we can compare them with the normal data (type '1') below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T18:35:49.532975Z",
     "start_time": "2018-12-05T18:35:47.782104Z"
    }
   },
   "outputs": [],
   "source": [
    "df_anomaly = pd.DataFrame(columns=['point','angle variance','type'])\n",
    "for index, item in enumerate(anomaly_data):\n",
    "    df_anomaly.loc[index] = [item, np.var(eval_angle_point(item, all_data)), 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the first ten entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T18:35:49.557722Z",
     "start_time": "2018-12-05T18:35:49.535323Z"
    }
   },
   "outputs": [],
   "source": [
    "df_anomaly.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a little disconcerting. The angle variance for many of the anomalies is quite high--higher even than the variance of the normal point we considered above. To complete our analysis, we also need the variance for the normal data, which we calculate below (labeling normal data as type '1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T18:36:07.914215Z",
     "start_time": "2018-12-05T18:35:49.560563Z"
    }
   },
   "outputs": [],
   "source": [
    "df_normal = pd.DataFrame(columns=['point','angle variance','type'])\n",
    "for index2, item2 in enumerate(normal_data):\n",
    "    df_normal.loc[index2] = [item2, np.var(eval_angle_point(item2, all_data)), 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T18:36:07.942516Z",
     "start_time": "2018-12-05T18:36:07.918565Z"
    }
   },
   "outputs": [],
   "source": [
    "df_normal.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Question:*** Why is the calculation for *normal_data* relatively slow? (Hint: What is the complexity of the algorithm?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Answer:*** The complexity of the algorithm is $O(N^{3})$ where $N$ is the number of points.\n",
    "\n",
    "To see this, think about how an angle is calculated. You need three points, and they should all be different. There are $N$ ways to choose the first point, $N-1$ ways to choose the second one and $N-2$ to choose the third. Swapping the second and third points gives the same angle, so the overall complexity is $N*(N-1)*(N-2)/2 = O(N^{3})$.\n",
    "\n",
    "Note that here we've split the algorithm into two parts: *eval_angle_point* calculates all angles for a given point [complexity $O(N^{2})$] and then the final loops above (for *df_anomaly* and *df_normal*) go through all points  [complexity $O(N)$]\n",
    "\n",
    "Going from 10 points for the anomalies to 100 points of normal data, increases the runtime by about a factor of $10^3=1000$, which leads to a noticeable slowdown. As we discussed, in the lectures, there are ways to speed up the algorithm, but even in the fancier versions, angle-based anomaly detection is slow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the two dataframes for the anomalies and the normal points and sort them in ascending order by angle variance. If our anomaly detection algorithm is perfect, then the first ten entries should all be anomalies (type 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T18:36:07.963675Z",
     "start_time": "2018-12-05T18:36:07.945875Z"
    }
   },
   "outputs": [],
   "source": [
    "df_all = df_anomaly.append(df_normal, ignore_index=True)\n",
    "df_all.sort_values(by=['angle variance']).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oops! We find only five of the ten anomalies and the two points with the lowest variance belong to the normal data. Clearly our algorithm needs some work. You will improve it in Exercise 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Depth-Based Anomaly Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we use depth-based anomaly detection to detect anomalies in simulated two-dimensional data. Just like angle-based anomaly detection, depth-based anomaly detection uses the geometric structure of the data, rather than an underlying probability distribution, to find anomalies. \n",
    "\n",
    "While it is clear what we mean by angle, the concept of depth must be defined. The general idea of depth is to assign a numerical value to each point. As with angle variance, points below a certain value of depth are considered anomalies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will take the depth to be the convex-hull peeling depth as defined below.\n",
    "(see also *Lesson 2* lecture and http://www.cs.tufts.edu/r/geometry/pdf/alenex06-deptheexplorer.pdf)\n",
    "\n",
    "The depth of a point X with respect to a dataset S (made of many points including X)\n",
    "is the level of the convex layer to which X belongs. The level of the convex layer is defined\n",
    "as follows: the points on the outer convex hull of S are designated level one and the points on the jth level (with j a positive integer) are the points on the convex hull of S after the points on all previous levels have been removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this definition, we carry out the depth-based anomaly detection as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Construct the convex hull for the data\n",
    "2. Label the points on the hull as depth 1\n",
    "3. Remove the depth 1 points\n",
    "4. Construct the the convex hull for the remaining data\n",
    "5. Label the points on the hall as depth 2\n",
    "6. Remove the depth 2 points\n",
    "7. Continue steps (4-6) incrementing the depth level by one each time\n",
    "8. Once there aren't enough points left to construct a convex hull, increment the depth and label the remaining points (if any) with the final depth value and end the convex hull construction\n",
    "9. The anomalies are those points whose depth values are below a pre-selected threshold \n",
    "\n",
    "Given the structure of the algorithm, we expect to implement it recursively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will illustrate the algorith with a 2D dataset. As we mentioned in the lecture, the depth-based slows down significantly as the dimensionality increases. We will work with a small dataset for visual clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T18:36:07.971608Z",
     "start_time": "2018-12-05T18:36:07.966876Z"
    }
   },
   "outputs": [],
   "source": [
    "# Select 30 random points from a 2D uniform distribution between zero and one\n",
    "np.random.seed(20)\n",
    "points = np.random.rand(30, 2)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T18:36:08.101697Z",
     "start_time": "2018-12-05T18:36:07.976040Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(points[:, 0], points[:, 1], 'o')\n",
    "plt.xlabel('x', fontsize=12)\n",
    "plt.ylabel('y', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will construct a series of convex hulls. Let's look at the one for all the data, which produces the points of depth 1.\n",
    "\n",
    "There is a ready-made Python function to construct the convex hull."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T18:36:08.112967Z",
     "start_time": "2018-12-05T18:36:08.104466Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.spatial import ConvexHull\n",
    "hull = ConvexHull(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T18:36:08.295530Z",
     "start_time": "2018-12-05T18:36:08.116791Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(points[:, 0], points[:, 1], 'o')\n",
    "plt.xlabel('x', fontsize=12)\n",
    "plt.ylabel('y', fontsize=12)\n",
    "for simplex in hull.simplices:\n",
    "    # In 2D, the simplicies are the lines connecting the points on the hull\n",
    "    plt.plot(points[simplex, 0], points[simplex, 1], 'k-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to implement the recursive convex hull construction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T18:36:08.305056Z",
     "start_time": "2018-12-05T18:36:08.298925Z"
    }
   },
   "outputs": [],
   "source": [
    "# The function *depth_calculator* takes three inputs: \n",
    "# the data; \n",
    "# a dictionary, which *depth_calculator* fills \n",
    "# with the coordinates of each point (key) and its corresponding depth (value);\n",
    "# a counter to keep track of the depth. Its initial value is the depth of the outermost hull\n",
    "\n",
    "def depth_calculator(data, dict, count):\n",
    "    next_data = []\n",
    "    if len(data) < 3: # in 2D, at least three points are needed to make a convex hull \n",
    "        for index, item in enumerate(data):\n",
    "            dict[tuple(item)] = count # assign depth to remaining points\n",
    "        print('All done! Need at least 3 points to construct a convex hull. ')  # End\n",
    "    else:\n",
    "        hull = ConvexHull(data) # Construct convex hull\n",
    "        for index, item in enumerate(data):\n",
    "            if item in data[hull.vertices]:\n",
    "                dict[tuple(item)] = count # assign depth to points on hull\n",
    "            else:\n",
    "                dict[tuple(item)] = count + 1.0 # assign depth+1 to points not on hull\n",
    "                next_data.append(item)\n",
    "        new_data =np.asarray(next_data) # create new data file of points not on hull \n",
    "        depth_calculator(new_data, dict, count + 1.0) # repeat on new data file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T18:36:08.320651Z",
     "start_time": "2018-12-05T18:36:08.309062Z"
    }
   },
   "outputs": [],
   "source": [
    "depth_dict = {} # empty dictionary for depth_calculator\n",
    "depth_calculator(points, depth_dict, 1.0) # initial hull has depth 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the points with their depth as an annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T18:36:08.336269Z",
     "start_time": "2018-12-05T18:36:08.328823Z"
    }
   },
   "outputs": [],
   "source": [
    "def depth_plot (dict):\n",
    "    xs,ys = zip(*dict.keys())\n",
    "    depth_label = list(dict.values())   \n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.title('Depth Labels', fontsize=20)\n",
    "    plt.xlabel('x', fontsize=15)\n",
    "    plt.ylabel('y', fontsize=15)\n",
    "    plt.scatter(xs, ys, marker = 'o',)\n",
    "    for label, x, y in zip(depth_label, xs, ys):\n",
    "        plt.annotate(label, xy = (x, y), xytext=(3,3), textcoords='offset points')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T18:36:08.625669Z",
     "start_time": "2018-12-05T18:36:08.346211Z"
    }
   },
   "outputs": [],
   "source": [
    "depth_plot(depth_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "You can see in this plot that points labeled '1.0' are those on the initial convex hull. Also, the final three points (depth 5.0) formed a triangle after which no points were left and the algorithm ended."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming we had set initially our anomaly threshold to be 1.0, we would have eight anomalies. Of course, in this case, there would be no need for a recursive approach--just take the points on the outermost convex hull. However, sometimes the depth is used as a score (a measure of how anomalous a point is). In such cases, it is useful to label all points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise #1\n",
    "\n",
    "This exercise refers to Section 1a, the analysis of the ozone data using the block maxima approach.\n",
    "\n",
    "A. Repeat the analysis above using a monthly maximum series (MMS) instead of an annual one.\n",
    "\n",
    "B. How do your results differ from the AMS case?\n",
    "\n",
    "C. What are the pros and cons of using MMS vs. AMS?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise #2\n",
    "\n",
    "This exercise refers to Section 2 (angle-based anomaly detection).\n",
    "\n",
    "A. Change the angle calculation so that it computed a modified angle between two vectors $\\bf{a}$ and $\\bf{b}$ as discussed in lectures:\n",
    "\n",
    "\\begin{equation}\n",
    "\\theta_{\\rm mod} = \\frac{\\bf{a} \\dot \\bf{b}}{a^{2}b^{2}}\n",
    "\\end{equation}\n",
    "\n",
    "where $a$ and $b$ are the absolute values of $\\bf{a}$ and $\\bf{b}$, respectively.\n",
    "\n",
    "B. Repeat the anomaly detection using the modified angle. Does this algorithm perform better?\n",
    "\n",
    "C. Has the complexity of the algorithm changed? If so, what is the new complexity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise #3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise refers to Section 3 (depth-based anomaly detection). \n",
    "\n",
    "A. Apply the depth-based anomaly detection algorithm to the data in section 2. \n",
    "\n",
    "B. How successful is the algorithm at finding the anomalies?\n",
    "\n",
    "C. How do your results compare with the angle-based anomaly detection algorithms (both versions)?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution #3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "In this assignment you should have learned: \n",
    "\n",
    "1. Two approaches to extreme value analysis: block maxima and peak over threshold\n",
    "2. How to apply angle-based anomaly detection \n",
    "3. How to apply depth-based anomaly detection \n",
    "\n",
    "Congratulations! This concludes the lesson."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Intel Anomaly Env",
   "language": "python",
   "name": "intel_anomaly"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
